\documentclass[11pt, a4paper]{article}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{float}
\usepackage[slovene]{babel}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{blkarray}
\usepackage{array}

\geometry{margin=1in}


\hyphenpenalty=10000

\begin{document}
    \newtheorem{theorem}{Izrek}[section]
    \newtheorem{definition}[theorem]{Definicija}
    \newtheorem{corollary}[theorem]{Posledica}
    \newtheorem{lemma}[theorem]{Lema}
    \newtheorem{proposition}[theorem]{Trditev}
    \newtheorem{example}[theorem]{Zgled}

    \newtheorem*{remark}{Opomba}


    \title{Optimizacijske metode}
    \author{Napisal Jure Pustoslemšek po zapiskih predavanj prof. dr. Matjaža Konvalinke}
    \date{Junij 2020}
    \maketitle

    \section{Optimizacijski problemi}

    Hocemo maksimizirati ali minimizirati realno funkcijo, definirano na neki mnozici.

        \begin{example}
            Minimum/maksimum funkcije \(f(x)=x^2-2x+4\) na \([-2,2]\)
            \par
            Uporabimo odvod funkcije.
        \end{example}

        \begin{example}
            Minimum/maksimum funkcije \(f(x,y)=x^2-y^2\) na \([-3,1]\times[0,2]\)
            \par
            Gledamo parcialne odvode in vrednosti funkcije na robu.
        \end{example}

        \begin{example}[Problem kmetije]
            \par
            Na kmetiji s \(50 ha\) pridelovalne površine želimo maksimizirati dobiček po spodnji tabeli

            \begin{center}
                \begin{tabular}{ c|c|c|c }
                    pridelek & ure dela & stroski & dobiček \\
                    \hline
                    pšenica & 60 & 400 & 240 \\
                    koruza & 80 & 600 & 400 \\
                    krompir & 100 & 480 & 320 \\
                \end{tabular}
            \end{center}
            
            Na voljo imamo 5.000 ur delovne sile in 24.000€ kapitala.
            
            \par
            Kaj nas zanima? Zanima nas, kako bomo razporedili različne pridelke po pridelovalni površini za najvecji dobiček.

            
            \begin{tabular}{ c c c }
                \(x_1\) & . . . & površina pšenice v \(ha\) \\
                \(x_2\) & . . . & površina koruze v \(ha\) \\
                \(x_3\) & . . . & površina krompirja v \(ha\) \\
            \end{tabular}
            
            
            
            Problem kmetije lahko matematično izrazimo takole: \\

            \begin{center}
                \begin{tabular}{ cc }
                    $max$ & \(
                        240x_1+400x_2+320x_3
                    \) \\
                    $p.p.$ & \(
                        x_1+x_2+x_3 \le 50
                    \) \\
                    & \(
                        60x_1+80x_2+100x_3 \le 5.000
                    \) \\
                    & \(
                        400x_1+600x_2+46x_3 \le 24.000
                    \) \\
                    & \(
                        x_1,x_2,x_3 \ge 0    
                    \) \\
                \end{tabular}
            \end{center}

            To je \emph{linearni program (LP)}.
        \end{example}

        \begin{example}
            Imamo \(2n\) jabolk, ki tehtajo \(w_1,...,w_{2n}\)
            \par
            Jabolka razvrstimo v dve košari tako, da je v vsaki \(n\) jabolk, in da sta teži kosar cim bolj podobni.

            \par
            Jabolka v 1. košari ponazorimo z množico A. Ker je v množici A natanko polovica jabolk, lahko 2. košaro ponazorimo z množico \(A^C\).

            \par
            Torej iščemo
            \begin{center}
                \[
                    min |\sum_{w_i \in A}{w_i} - \sum_{w_i \in A^C}{w_i}|
                \]
            \end{center}
            
            \par
            Poskusimo ta problem predstaviti kot LP: \\
            \(x_i\); \(i \in [2n]\) \\
            \(x_i = 1\), če damo \(i\)-to jabolko v levo košaro. \\
            \(x_i = -1\), če damo \(i\)-to jabolko v desno košaro. \\

            \begin{table}[h!]
                \[
                    \begin{array}{ cc }
                        min &
                            \displaystyle |\sum_{i=1}^{2n}{w_i x_i}| \\
                        
                        p.p. & 
                            \displaystyle \sum_{i=1}^{2n}{x_i} = 0 \\
                        &
                            x_i \in \{-1, 1\} \\
                    \end{array}
                \]
            \end{table}
            
            To \underline{ni} linearen program! Absolutna vrednost ni linearna.
        \end{example}

    \begin{definition}[Optimizacijski problem]
        Optimizacijski problem karakteriziramo kot \((min/max, f, \Omega)\), kjer je \(f\) kriterijska funkcija (tj. funkcija, ki jo zelimo minimizirati oz. maksimizirati), \(\Omega\) pa množica dopustnih rešitev, tj. vseh \(x\), ki ustrezajo danim pogojem. 
    \end{definition}
    Zanima nas \underline{optimalna vrednost}, tj. \(x^* \in \Omega\), za katerega je \(f(x^*)\) največji oz. najmanjši mozen.

    \par
    Optimizacijski problem je:
    \begin{itemize}
        \item Nedopusten, če \(\Omega = \emptyset\),
        \item Dopusten, če je \(\Omega \neq \emptyset\)
        
    \end{itemize}

    \par
    Dopustne probleme delimo na:
    \begin{itemize}
        \item Neomejen, če je \(\Omega\) neomejena, torej lahko vedno najdemo boljšo rešitev,
        \item Omejen, če je \(\Omega\) omejena
        
    \end{itemize}

    \par
    Omejene probleme pa še delimo na:
    \begin{itemize}
        \item Optimalen, če obstaja optimalna rešitev,
        \item Neoptimalen, če lahko vedno najdemo boljšo rešitev
    \end{itemize}



    \section{Linearno programiranje}

    \begin{definition}[Linearni program]
        Linearni program (LP) je optimizacijski problem, v katerem je kriterijska funkcija linearna, dopustna množica pa je podana z linearnimi enačbami in neenačbami.
    \end{definition}

    \begin{example}
        \[
            \begin{array}{ c c }
                min & 2x_1-3x_2+x_3 \\
                p.p. & x_1+x_2 \le 4 \\
                & 3x_1-x_2-x_3 \ge 1 \\
                & 2x_2-5x_3 = -1 \\
                & x_1 \ge 0 \\
                & x_3 \le 0 \\
            \end{array}
        \]
    \end{example}
    

    \begin{definition}[Standardna oblika LP]
        LP je v \underline{standarni obliki}, ce:
        \begin{enumerate}
            \item je tipa \(max\),
            \item vse omejitve so oblike \(g_j(x) \le b_j\) in
            \item za vse spremenljivke so omejene z \(x_i \ge 0\)
        \end{enumerate}
    \end{definition}

    \begin{figure}[h!]
        \[
            \begin{array}{ c c }
                max & c_1x_1+...+c_nx_n \\
                p.p. & a_{11}x_1+...+a_{1n}x_n \le b_1 \\
                & a_{21}x_1+...+a_{2n}x_n \le b_2 \\
                & . \\
                & . \\
                & . \\
                & a_{m1}x_1+...+a_{mn}x_n \le b_m \\
                & x_1,x_2,...,x_n \le 0 \\
                
            \end{array}
        \]
        \caption{LP v standardni obliki}
    \end{figure}

    \par
    LP v standardni obliki lahko izrazimo tudi v matričnem zapisu.  
    
    \begin{figure}[H]
        \[
            \begin{array}{ c c c c }
                x = 
                \begin{bmatrix}
                    x_1 \\
                    x_2 \\
                    . \\
                    . \\
                    x_n    
                \end{bmatrix}
                &
                c =
                \begin{bmatrix}
                    c_1 \\
                    c_2 \\
                    . \\
                    . \\
                    c_n
                \end{bmatrix}
                &
                b = 
                \begin{bmatrix}
                    b_1 \\
                    b_2 \\
                    . \\
                    . \\
                    . \\
                    b_m
                \end{bmatrix}
                &
                A = 
                \begin{bmatrix}
                    a_{ij}    
                \end{bmatrix}
                \in \mathbb{R}^{m \times n}
            \end{array}
        \]
    \end{figure}
    
    \par
    S temi oznakami lahko vsak LP (tudi takšne, ki niso v matrični obliki) zapišemo v matričnem zapisu:
    
    \begin{figure}[H]
        \[
            \begin{array}{ c c }
                max & c^Tx \\
                p.p. & Ax \le b \\
                & x \ge 0 \\
            \end{array}
        \]
        \caption{LP v standardni obliki (matrični zapis)}
    \end{figure}
    
    \begin{theorem} 
        Vsak LP lahko pretvorimo v ekvivalentni LP v standardni obliki.
    \end{theorem}

    \begin{proof}
        Pretvorbo poljubnega LP v standardno obliko opravimo v več korakih:
        \begin{enumerate}
            \item Minimizacijski problem je ekvivalenten maksimizacijskemu problemu negativne kriterijske funkcije z negativnimi pogoji.
            \[min f(x) \rightarrow max -f(x)\]
            \[a_{ij}x_i = b_j \rightarrow -a_{ij}x_i = -b_j\]
            \[a_{ij}x_i \le b_j \rightarrow -a_{ij}x_i \le -b_j\]
            \[a_{ij}x_i \ge b_j \rightarrow -a_{ij}x_i \ge -b_j\]

            \item Omejitve spremenljivk popravimo z vpeljavo novih spremenljvk:
            \begin{enumerate}[label*=\arabic*.]
                \item \(x_i \ge b_j\) zamenjamo z novo spremenljivko \(x_i'=x_i-b_j\), da dobimo omejitev \(x_i' \ge 0\)
                
                \item \(x_i \le b_j\) zamenjamo z novo spremenljivko \(x_i'=b_j-x_i\), da dobimo omejitev \(x_i' \ge 0\)
                
                \item Neomejene spremenljivke zamenjamo z dvema novima spremenljivkama, podanima z enačbo \(x_i=x_i'-x_i''\), novi spremenljivki pa sta omejeni z omejitvijo \(x_i',x_i'' \ge 0\) 
            \end{enumerate}

            \item Vsak pogoj oblike \(g_j(x) = b_j\) pretvorimo v dva pogoja:
            \par
            \(g_j(x) \le b_j\) in \(g_j(x) \ge b_j\)

            \item Pogoje oblike \(g_j(x) \ge b_j\) pomnožimo z \(-1\). S tem ga pretvorimo v \(-g_j(x) \le -b_j\)
        \end{enumerate}
    \end{proof}

    \paragraph{TODO: Konveksne množice}
    \clearpage


    \subsection{Simpleksna metoda}

    Simpleksna metoda zahteva LP v standardni obliki:
    \begin{figure}[h!]
        \[
            \begin{array}{ c c }
                max & c^Tx \\
                p.p. & Ax \le b \\
                & x \ge 0 \\
            \end{array}
        \]
        \caption{LP v standardni obliki (matrični zapis)}
    \end{figure}

    Označimo z \(n\) število spremenljivk in z \(m\) število neenakosti, tj. \(c,x \in \mathbb{R}^n \), \(b \in \mathbb{R}^m\) in \(A \in \mathbb{R}^{m \times n}\).


    \begin{example}[Problem kmetije]
        \begin{figure}[h!]
            \[
                \begin{array}{ c c }
                    max & 240x_1+400x_2+320x_3 \\
                    p.p. & x_1+x_2+x_3 \le 50 \\
                    & 60x_1+80x_2+100x_3 \le 5000 \\
                    & 400x_1+600x_2+480x_3 \le 2400 \\
                    & x_1,x_2,x_3 \ge 0 \\
                \end{array}
            \]
            \caption{LP problema kmetije v standardni obliki}
        \end{figure}

        Koeficiente v kriterijski funkciji in vsaki omejitvi posebej bomo delili z največjim skupnim deliteljem, da bomo dobili ekvivalenten LP z "lepšimi" številkami.

        \begin{figure}[h!]
            \[
                \begin{array}{ c c }
                    max & 3x_1+5x_2+4x_3 \\
                    p.p. & x_1+x_2+x_3 \le 50 \\
                    & 3x_1+4x_2+5x_3 \le 250 \\
                    & 10x_1+15x_2+12x_3 \le 600 \\
                    & x_1,x_2,x_3 \ge 0 \\
                \end{array}
            \]
            \caption{LP problema kmetije z "lepšimi" koeficienti}
        \end{figure}

        Vsako neenakost spremenimo v enakost z novo spremenljivko.

        \begin{figure}[h!]
            \[
                \begin{array}{ c }
                    x_4=50-x_1-x_2-x_3 \\
                    x_5=250-3x_1-4x_2-5x_3 \\
                    x_6=600-10x_1-15x_2-12x_3 \\
                    \hline
                    z=3x_1+5x_2+4x_3 \\
                \end{array}
            \]
            \caption{Prvi slovar simpleksne metode na problemu kmetije}
        \end{figure}

        To je \underline{prvi slovar} simpleksne metode.
    \end{example}
    

    V splošnem ima slovar \(m+1\) linearnih enačb in \(n+m+1\) spremenljivk.
    \(m\) spremenljivk izmed \(x_1,x_2,...,x_{n+m}\) in \(z\) je izraženih z ostalimi \(n\) spremenljivkami.

    \par
    Teh \(m\) spremenljivk, ki so izražene z ostalimi \(n\) spremenljivkami, imenujemo \underline{bazne spremenljivke}, ostale spremenljivke pa so \underline{nebazne} (spremenljivka \(z\) predstavlja vrednost kriterijske funkcije in ne spada med bazne ali nebazne spremenljivke).

    \par
    \(x_1,x_2,...,x_n\) so \underline{prvotne} spremenljivke.
    \par
    \(x_{n+1},x_{n+2}...,x_{n+m}\) so \underline{dodatne} spremenljivke.

    \par
    V prvem slovarju so bazne spremenljivke natanko dodatne spremenljivke, nebazne pa prvotne.

    \par
    Rečemo, da je slovar dopusten, če so vsi konstatni koeficienti baznih spremenljivk nenegativni. Prvi slovar je torej dopusten natanko takrat, ko je \(b \ge 0\). Če je \(b \ngeq 0\), uporabimo \textit{dvofazno simpleksno metodo} (kasneje). Zato zaenkrat predpostavimo \(b \ge 0\). V tem primeru je LP zagotovo dopusten, saj je \(x_1=x_2=...=x_n=0\) dopustna rešitev.
    \par
    \begin{remark}
        Vse spremenljivke (tako prvotne kot dodatne) so nenegativne.
    \end{remark}
    
    \par
    Z dopustnim slovarjem imamo \textit{bazno dopustno rešitev}: nebazne spremenljivke nastavimo na 0. \(f(x)\) je vrednost kriterijske funkcije, \(x\) pa so vrednosti prvotnih spremenljivk (nebazne smo nastavili na 0, bazne pa imajo vrednosti konstatnih koeficientov).
    
    \paragraph{TODO: opis algoritma simpleksne metode}


    \begin{theorem}[Osnovni izrek linearnega programiranja]
        \(\)\\
        \begin{enumerate}
            \item Če ima LP dopustno rešitev, ima tudi bazno dopustno rešitev.
            \item Če ima LP optimalno rešitev, ima tudi bazno optimalno rešitev.
            \item Velja natanko ena od možnosti za dani LP:
            \begin{itemize}
                \item LP je nedopusten.
                \item LP je neomejen.
                \item LP je optimalen.
            \end{itemize}
        \end{enumerate}
    \end{theorem}
    \clearpage



    \subsection{Dualnost pri linearnem programiranju}

    \textbf{Ideja:} Problem kmetije \(P\)
    \begin{figure}[h!]
        \[
            \begin{array}{ c c }
                max & 3x_1+5x_2+4x_3 \\
                p.p. & x_1+x_2+x_3 \le 50 \\
                & 3x_1+4x_2+5x_3 \le 250 \\
                & 10x_1+15x_2+12x_3 \le 600 \\
                & x_1,x_2,x_3 \ge 0 \\
            \end{array}
        \]
        \caption{LP problema kmetije}
    \end{figure}

    Zanimajo nas zgornje meje za kriterijsko funkcijo. 
    \par
    Neenakosti pomnožimo z \(y_2,y_2,...,y_m \ge 0\) in seštejemo:
    \[(x_1+x_2+x_3)y_1+(3x_1+4x_2+5x_3)y_2+(10x_1+15x_2+12x_3)y_3 \le 50y_1+250y_2+600y_3\]
    \[(y_1+3y_2+10y_3)x_1+(y_1+4y_2+15y_3)x_2+(y_2+5y_2+12y_3)x_3 \le 50y_1+250y_2+600y_3\]
    
    Če velja:
    
    \[
        \begin{array}{c}
            y_1+3y_2+10y_3 \ge 3 \\
            y_1+4y_2+15y_3 \ge 5 \\
            y_1+5y_2+12y_3 \ge 4 \\ 
        \end{array}
    \]
    je \(2x_1+5x_2+4x_3 \le 50y_1+250y_2+600y_3\).

    Dualni problem kmetije \(P'\):
    \begin{figure}[h!]
        \[
            \begin{array}{ c c }
                min & 50 y_1+250y_2+600y_3 \\
                p.p. & y_1+3y_2+10y_3 \ge 3 \\
                & y_1+4y_2+15y_3 \ge 5 \\
                & y_1+5y_2+12y_3 \ge 4 \\
                & y_1,y_2,y_3 \ge 0 \\
            \end{array}
        \]
        \caption{Dualni problem kmetije}
    \end{figure}

    V splošnem:
    \begin{definition}[Dualni LP]
        Za dani LP P v standardni obliki:
        \[
            \begin{array}{ c c }
                max & c^Tx \\
                p.p. & Ax \le b \\
                & x \ge 0 \\
            \end{array}    
        \]
        je \textbf{dualni problem} LP \(P'\)
        \[
            \begin{array}{ c c }
                min & b^Ty \\
                p.p. & A^Ty \ge c \\
                & y \ge 0 \\
            \end{array}
        \]
    \end{definition}

    \begin{proposition}
        Za poljuben LP \(P\) je \(P''=P\).
    \end{proposition}

    \begin{proof}
        Naj bo LP \(P\) v standardni obliki. Potem je  \(P'\) v standardni obliki
        \[
            \begin{array}{ c c }
                max & -b^Ty \\
                p.p. & (-A^T)y \le -c \\
                & y \ge 0 \\
            \end{array}
        \]
        \(P''\) je potem:
        \[
            \begin{array}{ c c }
                min & -c^Tx \\
                p.p. & (-A^T)^Tx \ge -b \\
                & x \ge 0 \\
            \end{array}
        \]
        kar po pretvorbi v standardno obliko postane enako kot P:
        \[
            \begin{array}{ c c }
                max & c^Tx \\
                p.p. & Ax \le b \\
                & x \ge 0 \\
            \end{array}
        \]
    \end{proof}

    \begin{remark}
        Če ima LP \(P\) \(n\) spremenljivk in \(m\) neenačb, ima njegov dualni LP \(P'\) \(m\) spremenljivk in \(n\) neenačb.
    \end{remark}

    \begin{theorem}[Šibki izrek o dualnosti (ŠID)]
        Naj bo \(P\) linearni program, \(P'\) njegov dualni LP ter \(x\) in \(y\) dopustni rešitvi za \(P\) in \(P'\). Ob uporabi oznak od prej je potem \(c^Tx \le b^Ty\).
    \end{theorem}

    \begin{proof}
        Brez izgube splošnosti predpostavimo, da je LP \(P\) v standardni obliki in opazimo, da je \(A^Ty \ge c\) in \(Ax \le b\). Ker velja tudi \(x \ge 0\) in \(y \ge 0\):
        \[c^Tx \le (A^Ty)^T = y^TAx \le y^Tb = b^Ty\]
    \end{proof}

    \begin{corollary}
        \label{optimalnost}
        Če velja \(c^Tx^* = b^Ty^*\) za dopustni rešitvi \(x^*,y^*\), je \(x^*\) optimalna rešitev za \(P\) in \(y^*\) optimalna rešitev za \(P'\).
    \end{corollary}

    \begin{proof}
        Ali lahko velja \(c^Tx > c^Tx^*\) za dopustno rešitev \(x\)?
        Potem velja \(c^Tx > b^Ty^*\), ampak po ŠID velja \(c^Tx \le b^Ty^*\). Protislovje.
        \par
        Kaj pa \(b^Ty < b^Ty^*\) za neko dopustno rešitev \(y\)?
        Potem velja \(b^Ty < c^Tx^*\), ampak po ŠID je \(b^Ty \ge c^Tx^*\). Znova smo prišli v protislovje. Torej \(x^*\) in \(y^*\) morata biti optimalni rešitvi za \(P\) in \(P'\).
    \end{proof}

    \begin{corollary}
        Če je \(P\) neomejen, je \(P'\) nedopusten.
    \end{corollary}

    \begin{theorem}[Krepki izrek o dualnosti (KID)]
        Če ime \(P\) optimalno rešitev \(x^*\), ima tudi \(P'\) optimalno rešitev \(y^*\) in velja \(c^Tx^* = b^Ty^*\).
    \end{theorem}

    \begin{proof}
        Ogledamo si zadnji slovar:
        \[z=v^* + \sum_{i=1}^{n+m} c_i^*x_i\]
        \[v^* = \sum_{i=1}^{n} c_i x_i^*\]
        \(c_i^*=0\), če je \(x_i\) bazna spremenljivka in \(c_i^* \le 0\), če je \(x_i\) nebazna spremenljivka.
        
        Nastavimo \(y_i^* = -c_{i+n}^*\) za \(i=1,...,m\)
        
        \begin{proposition}
            Naj bo \((y_1^*,...,y_m^*)\) dopustna rešitev za \(P'\). Potem je \[\sum_{i=1}^n c_i x_i^* = \sum_{i=1}^m b_i y_i^*\]
        \end{proposition}

        Če dokažemo to trditev, sta po posledici \ref{optimalnost} \(x^*\) in \(y^*\) optimalni.

        \[z = v^* + \sum_{i=1}^n c_i^* x_i + \sum_{i=n+1}^{n+m} c_i^* x_i\]
        
        \[z = v^* + \sum_{i=1}^n c_i^* x_i + \sum_{i=1}^m c_{i+n} x_{i+n}\]
       
        
        V naslednjem koraku uporabimo sledeči enačbi:
        
        \begin{center}
            \begin{tabular}{ c c c }
                \(c_{i+n}^* = -y_i^*\) & in & \(x_{i+n} = b_i - \sum_{j=1}^n a_{ij} x_j\)
            \end{tabular}
        \end{center}

        Prvo enačbo smo dobili iz nastavka za \(y^*\), druga pa je enačba za dopolnilno spremenljivko.
            
        
        \[z = v^* +\sum_{i=1}^n c_i^* x_i + \sum_{j=1}^m (-y_j^*)(b_j - \sum_{i=1}^n a_{ji} x_i)\]
        
        \[z = (v^* - \sum_{j=1}^m y_j^* b_j) + \sum_{i=1}^n x_i (c_i^* + \sum_{j=1}^m a_{ji} y_j^*)\]

        Po drugi strani pa je \(z = \sum_{i=1}^n c_i x_i\). Iz tega sledi:
        \[v^* - \sum_{j=1}^m b_j y_j^* = 0\]
        \[c_i^* + \sum_{j=1}^m a_{ij} y_j^* = c_i\]

        Ker je \(c_i^* \le 0\) in \(y_i^* = -c_{i+n}^*\), je \(y_i^* = -c_{i+n}^* \ge 0\). Iz tega sledi, da je \(y^*\) dopustna rešitev:

        \[\sum_{j=1}^m a_{ji} y_j^* \ge c_i\]
        \[y_1^*,...,y_m^* \ge 0\]

        Iz \(v^* - \sum_{j=1}^m b_j y_j^* = 0\) sledi, da je 
        \(v^* = \sum_{j=1}^m b_j y_j^*\).
        Ker pa je \(v^* = \sum_{i=1}^{n} c_i x_i^*\), sledi 
        \[\sum_{j=1}^m a_{ji} y_j^* = \sum_{j=1}^m b_j y_i^*\].
    \end{proof}

    \begin{example}
        \[
            \begin{array}{ r r c r c r l }
                max & 3x_1 & - & 15x_2 & + & 4x_3 \\
                p.p. & x_1 & + & x_2 & + & x_3 & \le 50 \\
                & 3x_1 & + & 4x_2 & - & 15x_3 & \le 250 \\
                & 10x_1 & + & 15x_2 & + & 12x_3 & \le 600 \\
                & \multicolumn{5}{r}{x_1,x_2,x_3} & \ge 0
            \end{array}
        \]
        
        Zadnji slovar: \(z=200 - \frac{1}{3}x_1 - \frac{1}{3}x_6\); \(x_4,x_5,x_6\) so dopolnilne spremenljivke. \\

        Kakšne so vrednosti dualne optimalne rešitve? \(y_j^*\) je enak koeficientu pri dodatni spremenljivki, ki pripada \(j\)-ti neenakosti. Iz podanega slovarja preberemo \(c_4^*=0,c_5^*=0,c_6^*=\frac{1}{3}\). Dualna optimalna rešitev je torej \({y^* = (0, 0, \frac{1}{3})}\).
    \end{example}

    V kakšni zvezi stav splošnem LP \(P\) in njegov dual \(P'\)?
    \begin{proposition}
        Za LP \(P\) in njegov dual \(P'\) velja natanko ena od možnosti:
        \begin{enumerate}
            \item oba sta optimalna
            \item oba sta nedopustna
            \item eden je neomejen, drugi pa nedopusten
        \end{enumerate}
    \end{proposition}

    \begin{proof}
        Po KID vemo, da je \(P\) optimalen natanko tedaj, ko je \(P'\) optimalen. \par
        Po ŠID vemo:
        \begin{itemize}
            \item \(P\) neomejen \(\Rightarrow\) \(P'\) nedopusten
            \item \(P'\) neomejen \(\Rightarrow\) \(P\) nedopusten
        \end{itemize}
    \end{proof}

    
    \begin{center}
        \begin{tabular}{ c|c c c }
            \(_P \backslash ^{P'}\) & nedopusten & neomejen & optimalen \\
            \hline \\
            nedopusten & \(\checkmark\) & \(\checkmark\) & \(\times\) \\
            \hline \\
            neomejen & \(\checkmark\) & \(\times\) & \(\times\) \\
            \hline \\
            optimalen & \(\times\) & \(\times\) & \(\checkmark\) \\
        \end{tabular}
        \captionof{table}{Tabela dopustnosti in optimalnosti dualnih LP}
    \end{center}
        

    \begin{theorem}[Izrek o dualnem dopolnjevanju (DD)]
        Naj bosta LP \(P,P'\) dualna problema ter \(x\) in \(y\) dopustni rešitvi za \(P\) in \(P'\). Potem sta \(x\) in \(y\) optimalna natanko tedaj, ko velja:
        \begin{itemize}
            \item \(\displaystyle \forall i=1,...,m: \sum_{j=1}^n a_{ij} x_j = b_i\) ali \(y_i=0\) 
            \item in \(\displaystyle \forall j=1,...,n: \sum_{i=1}^m a_{ij} y_j = c_j\) ali \(x_j=0\)
        \end{itemize}

        Ekvivalentno:
        \begin{itemize}
            \item \(\displaystyle \sum_{j=1}^n a_{ij} x_j < b_i \Rightarrow y_i = 0\)
            \item \(\displaystyle x_j = 0 \Rightarrow \sum_{i=1}^m a_{ij} y_i = c_j\)
        \end{itemize}
    \end{theorem}

    \begin{proof}
        \[L = \sum_{j=1}^n c_j x_j \le \sum_{j=1}^n (\sum_{i=1}^m a_{ij} y_i)x_j = \sum_{i=1}^m (\sum_{j=1}^n a_{ij} x_j)y_i \le \sum_{i=1}^m b_i y_i = D\]

        Prvo neenakost smo dobili z \(A^Ty \ge c\) in \(x \ge 0\), drugo pa z \(Ax \le b\) in \(y \ge 0\). KID pravi, da sta \(x,y\) optimalni natanko takrat, ko je \(L = D\), to je pa res natanko tedaj, ko velja:
        \begin{itemize}
            \item \(\displaystyle \forall j=1,...,m: c_j x_j = (\sum_{i=1}^m a_{ij} y_i)x_j\)
            \item \(\displaystyle \forall i=1,...,n: (\sum_{j=1}^n a_{ij} x_j)y_i = b_i y_i\)
        \end{itemize}

        Izpostavimo \(x_j\) v prvi in \(y_i\) v drugi enačbi:
        \begin{itemize}
            \item \(\displaystyle \forall j=1,...,m: x_j(\sum_{i=1}^m a_{ij} y_i - c_j) = 0\)
            \item \(\displaystyle \forall i=1,...,n: (\sum_{j=1}^n a_{ij} x_j - b_i)y_i = 0\)
        \end{itemize}
    \end{proof}

    \subsubsection{Ekonomski pomen dualnih spremenljivk}

    \begin{example}
        \(x_1^*=0,x_2^*=20,x_3^*=25\) je optimalna rešitev problema kmetije. 
        \[
            \begin{array}{ l l l }
                \text{površina:} & 45 & < 50 \\
                \text{delovna sila:} & 4.100  & < 5.000\\
                \text{kapital:} & 2.400 & = 2.400
            \end{array}
        \]

        Na kmetiji smo dobili idejo, da bi na banki vzeli kredit. Zanima nas, če in koliko se bo naš dobiček povečal, če s tem kreditom povečamo katero od dobrin.

        \[
            \begin{array}{ l l }
                \text{površina:} & y_1^*=0 \\
                \text{delovna sila:} & y_2^*=0 \\
                \text{kapital:} & y_3^*=\frac{1}{3}
            \end{array} 
        \]
    \end{example}

    Naslednji izrek nam bo povedal odgovor na to vprašanje.

    \begin{theorem}
        LP \(P\) naj ima neizrojeno bazno rešitev. To pomeni, da so vsi konstantni koeficienti v slovarju \(\neq 0\).
        Potem obstaja tak \(\epsilon > 0\), da ob pogoju \(| \triangle b_i | \le \epsilon\) za \(i=1,...,m\) velja \(\triangle z^* = \sum_{i=1}^{m} y_i^* \triangle b_i\), kjer je \((y_1^*,...,y_m^*)\) optimalna rešitev \(P^\prime\).
    \end{theorem}

    Izrek podajamo brez dokaza.

    \begin{example}
        Vrnimo se k kmetiji. Uporabimo izrek:
        \[
            \triangle z^* = \frac{1}{3} \triangle b_3
        \]
        \[
            | \triangle b_i | \le \epsilon\ za\ i=1,2,3
        \]

        Kredit vzamemo, če je strošek (tj. obresti, zavarovanje, itd.) vsakega evra kredita manjša od \(\frac{1}{3}\) evra. 
    \end{example}

    \subsubsection{Dual splošnega LP}

    \begin{theorem}
        Splošni LP \(P\) ima obliko:
        \[
            \begin{array}{r l l}
                max & \sum_{j=1}^{n} c_j x_j \\ 
                p.p. & \sum_{j=1}^{n} a_{ij} x_j \le b_i & i=1,...,m^\prime \\
                & \sum_{j=1}^{n} a_{ij} x_j = b_i & i=m^\prime + 1,...,m \\
                & x_j \ge 0 & j=1,...,n^\prime
            \end{array}
        \]
        \captionof{figure}{Splošni LP}
        

        Potem je njegov dualni LP \(P^\prime\):
        \[
            \begin{array}{r l l}
                min & \sum_{i=1}^{m} b_i y_i \\ 
                p.p. & \sum_{i=1}^{m} a_{ij} y_i \ge c_j & j=1,...,n^\prime \\
                & \sum_{i=1}^{m} a_{ij} y_i =  c_j & j=n^\prime + 1,...,n \\
                & y_i \ge 0 &  i=1,...,m^\prime
            \end{array}
        \]
        \captionof{figure}{Dual splošnega LP}

        Povedano z besedami:
        \begin{center}
            \begin{tabular}{r c l}
                neenanokosti v \(P\) & \(\longrightarrow\) & nenegativne spremenljivke v \(P^\prime\) \\
                enakosti v \(P\) & \(\longrightarrow\) & poljubne spremenljivke v \(P^\prime\) \\
                nenegativne spremenljivke v \(P\) & \(\longrightarrow\) & neenakosti v \(P^\prime\) \\
                poljubne spremenljivke v \(P\) & \(\longrightarrow\) & enakosti v \(P^\prime\) \\
            \end{tabular}
        \end{center}
        
            
    \end{theorem}

    \begin{proof}
        \(P\) pretvorimo v standardno obliko.
        \[
            \begin{array}{r r l l l}
                max & & \sum_{j=1}^{n^\prime} c_j x_j + \sum_{j=n^\prime + 1}^{n} c_j (x_j^\prime - x_j^{\prime \prime})) \\
                
                p.p. & & \sum_{j=1}^{n^\prime} a_{ij} x_j + \sum_{j=n^\prime + 1}^{n} a_{ij} (x_j^\prime - x_j^{\prime \prime})) & \le b_i & j=1,...,m \\

                & - & \sum_{j=1}^{n^\prime} a_{ij} x_j - \sum_{j=n^\prime + 1}^{n} a_{ij} (x_j^\prime - x_j^{\prime \prime})) & \le - b_i & j=m^\prime + 1,...,m \\

                & & x_j \ge 0 & & j=1,...,n^\prime \\
                & & x_j^\prime,x_j^{\prime\prime} \ge 0 & & j=n^\prime + 1,...,n \\
            \end{array}    
        \]

        LP v standardni obliki pa že znamo pretvoriti v dualni LP.
        \[
            \begin{array}{r r l l l}
                min & & \sum_{i=1}^{m^\prime} b_i y_i + \sum_{i=m^\prime + 1}^{m} b_i (y_i^\prime - y_i^{\prime \prime})) \\

                p.p. & & \sum_{i=1}^{m^\prime} a_{ij} y_i + \sum_{i=m^\prime + 1}^{m} a_{ij} (y_i^\prime - y_i^{\prime \prime})) & \ge c_i & i=1,...,n \\

                & - & \sum_{i=1}^{m^\prime} a_{ij} y_i - \sum_{i=m^\prime + 1}^{m} a_{ij} (y_i^\prime - y_i^{\prime \prime})) & \ge - c_i & i=n^\prime,...,n \\

                & & y_i \ge 0 & & i=1,...,m^\prime \\
                & & y_i^\prime, y_i^{\prime \prime} \ge 0 & & i=m^\prime + 1,...,m \\
             \end{array}
        \]
    \end{proof}


    \pagebreak
    \section{Matrične igre}

    Naslednji optimizacijski problem, ki si ga bomo ogledali, je v bistvu tip igre. Z tovrstnimi problemi se sicer ukvarja teorija iger, mi pa si bomo matrične igre ogledali kot optimizacijski problem. Iz tega vidika gre za poseben tip linearnega programa, ki pa ima nekatere lepe lastnosti, zaradi katerih lahko problem lažje formuliramo in ga poenostavimo, preden jo rešimo z dvofazno simpleksno metodo.

    \begin{definition}
        Matrična igra je igra za 2 igralca, pri kateri ima 1. igralec na izbiro \(n\) izbir, 2. igralec \(m\) izbir, izid igre pa določa plačilna matrika
        \[A=[a_{ij}]_{\substack{ i=1,...,n \\ j=1,...,m} } \in \mathbb{R}^{n \times m}\]
        \(a_{ij}\) je znesek, ki ga 2. igralec plača 1. igralcu, če 1. igralec uporabi \(i\)-to strategijo, 2. igralec pa \(j\)-to strategijo. V primeru, ko je \(a_{ij} < 0\), 1. igralec plača \(-a_{ij}\) 2. igralcu.
    \end{definition}

    \begin{example}
        Kamen-papir-škarje
        \[
            \begin{blockarray}{c c c c}
                & \text{K} & \text{P} & \text{Š} \\
                \begin{block}{c [c c c]}
                    \text{K} & 0 & -1 & 1 \\
                    \text{P} & 1 & 0 & -1 \\
                    \text{Š} & -1 & 1 & 0 \\
                \end{block}
            \end{blockarray}
        \]
    \end{example}

    \begin{example}
        Igra Blotto
        \par
        Polkovnik Blotto ima 4 bataljone, major Clark pa 3 bataljone. Razporedita jih med 2 strateški točki. V vsaki točki zmaga tisti, ki je tja postavil več bataljonov. Poraženec plača \(k+1\), kjer je \(k\) število bataljonov poraženca na strateški točki. \\
        Blotto ima 5 možnih strategij: \((4,0),(3,1),(2,2),(1,3),(0,4)\) \\
        Clark ima 4 možne strategije: \((3,0),(2,1),(1,2),(0,3)\)
        
        \[
            A=
            \begin{bmatrix}
                4 & 2 & 1 & 0 \\
                1 & 3 & 0 & -1 \\
                -2 & 2 & 2 & -2 \\
                -1 & 0 & 3 & 1 \\
                0 & 1 & 2 & 4 \\
            \end{bmatrix}    
        \]
    \end{example}

    Predpostavimo, da igralca igrata po načelu najmanjšega tveganja. Definiramo \(M_1 =\) vrednost najmanjšega tveganja za igralca 1 in \(M_2 =\) vrednost najmanjšega tveganja za igralca 2. 
    \[
        \begin{array}{*3{>{\displaystyle}l}}
            M_1 & = & \max_i \min_j a_{ij} \\
            M_2 & = & \min_j \max_i a_{ij}
        \end{array}  
    \]

    Za igro Blotto je \(M_1=0\) in \(M_2=3\).
    
    \begin{proposition}
        Za vsako matrično igro velja \(M_1 \le M_2\).
    \end{proposition}

    \begin{proof}
        Vzemimo poljubno \(i\)-to vrstico in poljuben \(j\)-ti stolpec.
        \[
            \min_{j^\prime} a_{ij^\prime} \le a_{ij} \le \max_{i^\prime} a_{i^\prime j}    
        \]
    \end{proof}

    \begin{definition}[Sedlo matrične igre]
        \(a_{i_0 j_0}\) je \textbf{sedlo}, če je najmanjši v vrstici in največji v stolpcu.
    \end{definition}

    \begin{proposition}
        Matrična igra \(A\) ima sedlo natanko takrat, ko je \(M_1 = M_2\).
    \end{proposition}

    \begin{proof}[Dokaz \(\implies\)]
        \(a_{i_0 j_0}\) naj bo sedlo matrične igre \(A\). Potem je:
        \[
            \begin{array}{*2{>{\displaystyle}c}}
                M_1 = \max_i \min_j a_{ij} & M_2 = \min_j \max_i a_{ij}
            \end{array}
        \]
        \[
            M_1 \ge \min_j a_{i_0 j} = a_{i_0 j_0} = \max_i a_{i j_0} \ge M_2 \ge M_1
        \]

        Ker je \(M_1 \le M_2\) in hkrati \(M_1 \ge M_2\), torej je \(M_1 = M_2\).
    \end{proof}

    \begin{proof}[Dokaz \(\Longleftarrow\)]
        Naj bo \(M_1 = M_2\). 
        \par
        \(M_1\) (maksimum najmanjših vrednosti v stolpcih) je dosežen v vrstici \(i_0\).
        \[
            M_1 = \min_j a_{i_0 j}    
        \]
        Minimum je dosežen v stolpcu \(j_0\), torej je \(M_1 = a_{i_0 j_0}\). \\
        
        \(M_2\) (minimum največjih vrednosti v vrsticah) je dosežen v stolpcu \(j_0^\prime\).
        \[
            M_2 = \max_i a_{i j_0^\prime}   
        \]
        Maksimum je dosežen v vrstici \(i_0^\prime\), torej je \(M_2 = a_{i_0^\prime j_0^\prime}\). Pokazati želimo, da je \(a_{i_0 j_0^\prime}\) sedlo. \(a_{i_0 j_0^\prime}\) je v isti vrstici kot \(M_1 = a_{i_0 j_0}\), ki pa je najmanjši v svoji vrstici, torej je \(M_1 \le a_{i_0 j_0^\prime}\). Podobno, \(a_{i_0 j_0^\prime}\) je v istem stolpcu kot \(M_2 = a_{i_0^\prime j_0^\prime}\), ki je največji v svojem stolpcu, zato je \(a_{i_0 j_0^\prime} \le M_2\). Ker pa je \(M_1 = M_2\), je \(a_{i_0 j_0^\prime} = M_1 = M_2\), kar pomeni, da je najmanjši v svoji vrstici (ker je enak \(M_1\)) in največji v svojem stolpcu (ker je enak \(M_2\)), torej je sedlo.
    \end{proof}

    \begin{corollary}
        Če je \(a_{i_0 j_0}\) sedlo, je \(i_0\) optimalna strategija za 1. igralca in \(j_0\) optimalna strategija za 2. igralca.
    \end{corollary}

    Ta posledica sledi iz tega, da je sedlo enako \(M_1\), ki je najbolj varna strategija za 1. igralca, in je hkrati enako \(M_2\), ki je najbolj varna strategija za 2. igralca.

    \subsection{Mešana strategija}

    Pri mešani strategiji vsak igralec izbere verjetnosti, da uporabi vsako izmed svojih strategij.

    \[
        \begin{array}{l c l r}
            \text{1. igralec:} & (x_1,...,x_n) & x_i \ge 0 \sum_{i=1}^n x_i = 1 \\
            \text{2. igralec:} & (y_1,...,y_m) & y_j \ge 0 \sum_{j=1}^m y_j = 1 \\
        \end{array}    
    \]

    Z \(E(x,y)\) označimo pričakovani 1. igralca, če 1. igralec uporabi mešano strategijo \(x\), 2. igralec pa mešano strategijo \(y\).
    \[
        E(x,y) = \sum_{j=1}^{m} \sum_{i=1}^{n} a_{ij} x_i y_i = \sum_{i=1}^{n} x_i (\sum_{j=1}^m a_{ij} y_j) = \sum_{j=1}^n x_i (Ay)_i = x^T A y = (A^T x)^T y    
    \]

    Recimo, da se 1. igralec odloči uporabiti neko strategijo \(x\). Ker ne ve, kakšno strategijo bo uporabil 2. igralec, je zanj najbolj smiselno pogledati, koliko bo dobil oz. izgubil v najslabšem primeru:
    \[
        \min_y x^T A y    
    \]

    Torej, če hoče 1. igralec igrati z optimalno strategijo, tj. tisto, s katero je najslabši primer najboljši možen.
    \[
        \max_x \min_y x^T A y    
    \]

    Za 2. igralca izpeljemo podobno formulo za njegovo optimalno strategijo:
    \[
        \min_y max_x x^T A y    
    \]
    
    \begin{proposition}
        Naj bo \(A\) matrična igra. Potem velja
        \[
            \max_x \min_y x^T A y \le \min_y \max_x x^T A y    
        \]
    \end{proposition}

    \begin{proof}
        Izberemo strategiji \(x,y\).
        \[
            \min_{y^\prime} x^T A y^\prime \le x^T A y \le \max_{x^\prime} x^{\prime^T} A y    
        \]
    \end{proof}

    \begin{lemma}
        \[
            \min_y x^T A y = \min_{j=1,...,m} \sum_{i=1}^n a_{ij} x_i    
        \]
    \end{lemma}

    \begin{proof}
        Označimo \(s = \min_y \sum_{j=1}^n a_{ij} x_i\).
        \[
            y = (0,...,1,...,0)    
        \]
        \(y\) je \textbf{Čista strategija}. Označimo \(y_{j^\prime} = 1\). Za vse ostale \(j\) je \(y_j = 0\). 
        \[
            x^T A y = \sum_{i=1}^n \sum_{j=1}^m a_{ij} x_i y_j 
        \]
        Ker je \(y_j=0\) za vse \(j \neq j^\prime\):
        \[
            \sum_{i=1}^n \sum_{j=1}^m a_{ij} x_i y_j  = \sum_{i=1}^n a_{ij} x_i    
        \]
        \[
            \min_y x^T A y \le \sum_{i=1}^n a_{ij} x_i \text{ za } \forall j=1,...,m    
        \]
        To pomeni, da je 
        \[
            \min_y x^T A y \le s = \min_y \sum_{j=1}^n a_{ij} x_i 
        \]


        Pokažimo še nasprotno neenakost. Upoštevamo, da je \(y \ge 0\) in da je \(y\) čista strategija:
        \[
            x^T A y = \sum_{j=1}^n (\sum_{i=1}^m a_{ij} x_i) y_j \ge \sum_{j=1}^m s\ y_i = s
        \]
        Iz tega se vidi:
        \[
            \min_y x^T A y \ge s
        \]
    \end{proof}

    \begin{remark}
        Na izbrano strategijo 1. igralca se 2. igralec lahko optimalno brani s čisto strategijo.
    \end{remark}

    \begin{lemma}
        \[
            \max_x x^T A y = \max_{i=1,...,n} \sum_{j=1}^m a_{ij} y_j
        \]
    \end{lemma}

    Ta lema ima enak dokaz in opombo.
    \par
    Igro bi radi formulirali kot LP. S stališča 1. igralca:
    \[
        \max_x \min_y x^T A y = \max_x \min_{j=1,...,m} \sum_{i=1}^n a_{ij} x_i    
    \]
    To ni linearna funkcija, vendar lahko s trikom to popravimo.
    \[
        \begin{array}{r l l}
            max & s \\
            p.p. & s \le \sum_{i=1}^n a_{ij} x_i & j=1,...,m \\
            & \sum_{i=1}^n x_i = 1 \\
            & x_i \ge 0 & i=1,...,n
        \end{array}  
    \]

    Podobno pogledamo še iz stališča 2. igralca:
    \[
        \begin{array}{r l l}
            min & t \\
            p.p. & t \ge \sum_{j=1}^m a_{ij} y_j & i=1,...,n \\
            & \sum_{j=1}^m y_j = 1 \\
            & x_i \ge 0 & j=1,...,m
        \end{array}  
    \]

    Lahko vidimo, da sta problema dualna. Oba problema sta dopustna - izberemo dve poljubni čisti strategiji. Po KID sta oba problema optimalna, njuni optimalni vrednosti pa sta enaki.

    \begin{theorem}[Izrek o minimaksu]
        Za poljubno matrično igro je
        \[
            \max_x \min_y x^T A y = \min_y \max_x x^T A y    
        \]
    \end{theorem}
    Tej vrednosti pravimo \textbf{vrednost} ali \textbf{strateško sedlo} igre. Igra je \textbf{poštena}, če je njena vrednost 0.

    \par
    Matrično igro rešimo z dvofazno simpleksno metodo. Preverjanje optimalnosti je enostavno:
    \[
        \begin{array}{l c l}
            x,y\ \text{dopustna} \\
            x,y\ \text{optimalna} & \Longleftrightarrow & \min_j \sum_{i=1}^m a_{ij} x_i = \max_i \sum_{j=1}^m a_{ij} y_j
        \end{array}    
    \]

    \begin{proposition}
        Če ima \(A\) sedlo \(a_{i_0 j_0}\), potem je \(i_0\) optimalna strategija za 1. igralca, \(j_0\) pa za drugega.
    \end{proposition}

    \begin{proof}
        \[
            s = \min_j \sum_{i=1}^n a_{ij} x_i = \min_j a_{i_0 j} = a_{i_0 j_0}    
        \]

        \[
            t = \max_i \sum_{j=1}^m a_{ij} y_j = \max_i a_{i j_0} = a_{i_0 j_0}    
        \]
    \end{proof}

    \begin{definition}
        Matrična igra \textbf{simetrična}, če je \(A^T = -A\)
    \end{definition}
    
    \begin{proposition}
        Simetrična igra je poštena. 
    \end{proposition}

    \begin{proof}
        Za dokaz bomo potrebovali naslednjo izpeljavo:
        \[
            x^T A y = x^T (-A^T) y = - (Ax)^T y = - y^T A x
        \]

        Naj bo \(v\) vrednost simetrične matrične igre. Potem je
        \[
            v = \min_y \max_x x^T A y = \max_x \min_y (-y^T A x) = - \max_y \min_x y^T A x = - \max_x \min_y x^T A y = -v
        \]

        Ker je \(v = -v\), mora biti \(v = 0\).
    \end{proof}

    \begin{definition}
        Naj bosta  \(a,b \in \mathbb{R}^n\). Rečemo, da \(a\) \textbf{dominira} \(b\), če je \(a_i \ge b_i\) za vse \(i\).
    \end{definition}

    \begin{proposition}
        Če \(i\)-ta vrstica plačilne matrike dominira \(i^\prime\)-to vrstico, se vrednost igre ne spremeni, če izbrišemo vrstico \(i^\prime\).
    \end{proposition}

    \begin{proposition}
        Če \(j\)-ti stolpec plačilne matrike dominira \(j^\prime\)-ti stolpec, se vrednost igre ne spremeni, če izbrišemo stolpec \(j\).
    \end{proposition}

    Torej lahko brišemo večje stolpce in manjše vrstice brez vpliva na vrednost igre.


    \pagebreak
    \section{Problem razvoza (angl. Transshipment problem)}

    Imamo mesta, kjer se neka surovina proizvaja ali porablja, poti med mesti in ceno vsake poti na enoto surovine. Naša naloga je zadovoljiti potrebe po surovini s čim manjšimi stroški. Matematično ta problem predstavimo z grafom:
    \par
    Naj bo \(G = (V,E)\) usmerjen graf. Za vsako vozlišče \(v \in V\) imamo dano utež \(b_v \in \mathbb{R}\). Če je \(b_v > 0\), je vozlišče porabnik, če je pa \(b_v < 0\), pa je proizvajalec.
    \par
    Za vsako povezavo \(e \in E\) imamo dano utež \(c_e \in \mathbb{R}\), ki predstavlja ceno prevoza na enoto surovine.
    \[
        \text{Predpostavka: } \sum_{v} b_v = 0   
    \]
    \[
        \begin{array}{l c r}
            \text{Iščemo:} & x_e \ge 0, & \displaystyle min\ \sum_{e \in E} c_e x_e
        \end{array}
    \]

    V vsakem vozlišču velja Kirchhoffov zakon:
    \[
        \sum_{Konec(e)=v} x_e\ - \sum_{Zacetek(e)=v} x_e = b_v    
    \]

    Sestavimo \textbf{incidenčno matriko} \(A\):
    \[
        a_{ve} = 
        \begin{cases}
            0 & v \text{ ni vozlišče } e \\
            1 & Konec(e)=v \\
            -1 & Zacetek(e)=v \\
        \end{cases}
    \]

    Problem razvoza (PR) izrazimo kot LP:
    \[
        \begin{array}{l l}
            min & c^T x \\
            p.p. & Ax = b \\
            & x \ge 0
        \end{array}
    \]

    PR lahko rešimo z dvofazno simpleksno metodo, lahko pa jo rešimo tudi s simpleksno metodo na omrežjih.

    \subsection{Simpleksna metoda na omrežjih}

    Simpleksno metodo na omrežjih začnemo, podobno kot navadno simpleksno metodo ali drugo fazo dvofazne simpleksne metode, tj. z dopustno rešitvijo, katero izboljšujemo, dokler jo je možno izboljšati. Pri simpleksni metodi na omrežjih bomo za začetno dopustno rešitev potrebovali vpeto drevo \(T\) (za vpeto drevo velja \(V(T)=V(G)\)). Potrebovali bomo tudi dopustno rešitev \((x_e)_{e \in E}\), za katero bo veljalo \(x_e=0\) za \(e \notin T\). Takšni rešitvi pravimo \textbf{drevesna dopustna rešitev (ddr)}. Postopek, kako priti do začetne \(ddr\) oz. pokazati, da je problem nedopusten, bomo videli malo kasneje. Kasneje bomo tudi dokazali, da je optimalna rešitev vedno drevesna.
    \par
    

    \begin{enumerate}
        \item Za drevesno dopustno rešitev izberemo poljubno vozlišče in izračunamo razvozne cene od tega do vseh drugih vozlišč.
            \[
                y_1 = 0    
            \]
            \[
                y_j = y_i + c_{ij} \text{ za vsak } ij \in T   
            \]
            Ta sistem enačb je enoličen, ker je \(T\) drevo. Ker je \(T\) vpeto drevo, smo s tem sistemom enačb prišli do vseh vozlišč v originalnem grafu \(G\).

        \item Poiščemo povezavo \(ij \in G \backslash T\), za katero je \(y_i + c_{ij} < y_j\). To je \textbf{vstopajoča povezava}, ki jo dodamo v drevo \(T\).
        
        \item Ko smo v drevo dodali povezavo \(e\), smo dobili natanko en cikel. V tem ciklu ločimo povezave glede na smer povezave \(e\):
            \begin{itemize}
                \item povezave v smeri povezave \(e\) so \textit{preme},
                \item povezave v nasprotni smeri so \textit{obratne}
            \end{itemize}
        
            Odstraniti hočemo eno povezavo \(f\) iz tega cikla, da dobimo nov \textit{ddr}. Tej povezavi rečemo \textbf{izstopajoča povezava}. Ker \(f\) ne bo v novem drevesu, bo \(x^\prime_f=0\). Da bo nov \textit{ddr} ustrezal Kirchhoffovemu pogoju, moramo premim povezavam na ciklu povečati pretok za \(x_f\), obratnim pa ga zmanjšati za \(x_f\). Ker hočemo povezavi \(f\) zmanjšati \(x_f\), da bomo za \(f\) izbrali eno izmed obratnih povezav. Da pa bo še vedno \(x_e \ge 0\) za vse \(e \in E(G)\), moramo za \(f\) izbrati obratno povezavo z najmanjšim \(x_f\). Torej bomo premim povezavam povečali, obratnim pa zmanjšali pretok za
            \[
                t = \min \{\ x_f\ |\ f\ \text{obratna povezava}\ \}    
            \]
    \end{enumerate}

    Postopek ponavljamo, dokler v 2. koraku ne najdemo vstopajoče povezave. To je analogno koncu navadne simpleksne metode, ki se tudi konča takrat, ko ne najdemo nobene vstopajoče spremenljivke.

    \begin{proposition}
        Naj bo \(x\) \textit{ddr} in \(y\) vektor cen razvoza za PR. Potem je \(c^T x = b^T y\). 
    \end{proposition}

    \begin{proof}
        PR je omejen z \(Ax=b\), torej je \(b^T y = (Ax)^T y = x^T (A^T y)\). \(A^T\) ima v vrstici povezave \(ij\) vrednost 1 v stolpcu vozlišča \(j\) in vrednost -1 v stolpcu vozlišča \(i\), povsod drugje pa ima vrednost 0\, zato je \((A^T y)_{ij}=y_j-y_i\). Iz enačb, ki določajo cene razvoza in predpostavke, da je \(x\) \textit{ddr}, imamo še enačbi:
        \[
            \begin{array}{l r c l}
                \forall ij \in T: & y_j - y_i & = & c_{ij} \\
                \forall ij \notin T: &  x_{ij} & = & 0   
            \end{array}
        \]
        
        Iz teh enačb izpeljemo
        \[
            b^T = x^T (A^T y) = \sum_{ij \in E} x_{ij}\ (y_j - y_i) = \sum_{ij \in E} x_{ij}\ c_{ij} = c^T x    
        \]
    \end{proof}

    \begin{proposition}
        Naj bo \(x\) \textit{ddr} in \(y\) vektor cen razvoza. Če za vse \(ij \in E\) velja \(y_i + c_{ij} \ge y_j\), je \(x\) optimalna rešitev.
    \end{proposition}

    \begin{proof}
        \(x\) je dopustna rešitev, torej \(Ax = b\) in \(x \ge 0\). Po predpostavki je \((A^T y)_{ij} = y_j - y_i \le c_{ij}\).
        \[
            c^T x \ge (A^T y)^T x = y^T (Ax) = y^T b = c^T x     
        \]
    \end{proof}

    \begin{proposition}
        Če je \(G\) povezan graf (kot neusmerjen graf), potem obstaja dopustna rešitev za PR natanko tedaj, ko obstaja \textit{ddr}.
    \end{proposition}

    \begin{proof}
        (\(\Longleftarrow\)) Očitno. Dokažimo (\(\implies\)): \\
        Predpostavimo, da na dopustni rešitvi obstaja cikel \(C\), na katerem je \(x_e > 0\) za vse \(e \in C\). Izberemo smer in razvoz povezave z najmanjšim \(t = x_e\). Povezavam v smeri povezave \(e\) zmanjšamo razvoz za \(t\), povezavam v nasprotni smeri pa razboz povečamo za \(t\). Dobili smo \textit{ddr}.
    \end{proof}

    \begin{remark}
        Gozdu (grafu, v katerem so povezane komponente drevesa) dodamo povezave z razvozom \(x_{ij} = 0\) med povezanimi komponentami, da dobimo drevo.
    \end{remark}

    Če smo med postopkom simpleksne metode na omrežjih dobili cikel s samimi premimi povezavami, očitno ne morem izbrati izstopajoče povezave, ker bi ta morala biti obratna. V tem primeru smo dobili \textbf{negativen cikel}, tj. cikel, na katerem je vsota cen negativna. Če surovino prevozimo skozi cikel, smo s prevozom zaslužili! To pomeni, da lahko \(c^T x\) postane poljubno negativen, če surovino poljubnokrat peljemo skozi ta cikel. To pa pomeni, da je podan PR neomejen problem, če je dopusten.

    \par
    Tudi pri simpleksni metodi na omrežjih lahko pride do ciklanja. To lahko preprečimo s Cunninghamovim pravilom (lahko ga razumemo kot analognega pravilu najmanjšega indeksa):
    \par
    Naj bo \(e = ij\) vstoppajoča povezava. Neko vozlišče v \(G\) označimo kot koren in ga poimenujemo \(r\). Naj bo \(v\) prvo vozlišče na poti od \(r\) do \(i\), ki je na ciklu. Med obratnimi povezavami na ciklu z najmanjšim razvozom izberemo tisto, ki je prva na poti od \(v\) proti \(e\).

    \subsection{Dvofazna simpleksna metoda na omrežjih}

    Ostaja nam še vprašanje - kako najdemo prvotno drevesno dopustno rešitev oz. dokažemo, da je problem nedopusten? Odgovor na to vprašanje je dvofazna simpleksna metoda na omrežjih, ki je povsem analogna navadni dvofazni simpleksni metodi.
    \par
    Izberemo poljubno vozlišče \(r\) in ga imenujemo koren. Za vsako vozlišče \(v \neq r\):
    \begin{itemize}
        \item \(b_v \ge 0\): dodamo povezavo \(rv\), če je še ni,
        \item \(b_v < 0\): dodamo povezavo \(vr\), če je še ni
    \end{itemize}

    Te dodatne povezave imenujemo \textbf{umetne povezave}. Cene umetnih povezav nastavimo na \(c_e=1\), cene prvotnih pa na \(c_e=0\). Za ta problem obstaja \textit{ddr}: Povezavam \(rv\), kjer je \(b_v \ge 0\), nastavimo \(x_{rv}=b_v\), povezavam \(vr\), kjer je \(b_v < 0\), pa nastavimo \(x_{vr}=b_v\). Ostalim povezavam nastavimo \(x_{ij}=0\). Takšen \(x\) očitno ustreza pogoju \(x \ge 0\). Preverimo, da velja tudi Kirchhoffov zakon:
    \[
        \begin{array}{l l r}
            v \neq r, & b_v \ge 0 & \displaystyle \sum_{K(e)=v} x_e -\ \sum_{Z(e)=v} x_e = b_v - 0 = b_v \\
            v \neq r, & b_v \ge 0 & \displaystyle \sum_{K(e)=v} x_e -\ \sum_{Z(e)=v} x_e = 0 - (-b_v) = b_v \\
        \end{array}
    \]

    Za \(v=r\) tudi velja:
    \[
        \sum_{K(e)=r} x_e -\ \sum_{Z(e)=r} x_e = \sum_{\substack{v \neq r \\ b_v < 0}} (-b_v) - \sum_{\substack {v \neq r \\ b_v \ge 0}} b_v = - \sum_{v \neq r} b_v = b_r\ \text{(ker je } \sum_{v \in V} b_v = 0 \text{)}
    \]

    Ker je \(c_e \in \{ 0,1 \}\), je \(c^T x \ge 0\), zato problem ni neomejen, torej je optimalen. Na tem problemu uporabimo simpleksno metodo na omrežjih.

    \begin{proposition}
        Če je optimalna vrednost tega problema enaka 0, je prvotni problem dopusten.
    \end{proposition}

    \begin{proof}[Dokaz \(\implies\)]
        Na umetnih povezavah je \(c_e=1\), torej je \(x_e=0\), ker je \(c^T x = 0\). Vzamemo \(x_e\) na prvotnih povezavah in imamo dopustno rešitev, saj za prvotne povezave veljajo enake omejitve kot pri prvotnem problemu.
    \end{proof}

    \begin{proof}[Dokaz \(\Longleftarrow\)]
        Predpostavimo, da je prvotni problem dopusten. Potem ima dopustno rešitev na prvotnih povezavah. te vrednost \(x_e\) ohranimo, na umetnih povezavah pa nastavimo \(x_e=0\). Ker je \(c_e=0\) za prvotne povezave, je \(c^T x = 0\). Ker je \(c^T x \ge 0\), imamo optimalno rešitev.
    \end{proof}

    Ko se 1. faza ustavi, imamo tri možnosti:
    \begin{itemize}
        \item \(T^\ast\) nima umetne povezave. Potem je \(T^\ast\) \textit{ddr} za prvotni problem.
        \item \(T^\ast\) vsebuje vsaj eno umetno povezavo z razvozom \(x_e > 0\). Potem je optimalna vrednost problema 1. faze \(> 0\). Prvotni problem ni dopusten.
        \item \(T^\ast\) vsebuje umetne povezave, in vse z ničelnim razvozom. Odstranimo vse umetne povezave in dobimo gozd. Gozdu dodamo prvotne povezave z ničelnim razvozom in dobimo drevesno dopustno rešitev.
    \end{itemize}

    2. faza je običajna simpleksna metoda na omrežjih.

    \subsection{Dual PR}

    LP za PR je:
    \[
        \begin{array}{l r c l}
            min & c^T x \\
            p.p. & Ax & = & b \\
            & x & \ge & 0
        \end{array}    
    \]
    
    \[
        \begin{array}{l r c r}
            max & (-c)^T x \\
            p.p. & (-A)x & = & -b \\
            & x & \ge & 0
        \end{array}    
    \]

    Dualni LP za PR (\(PR^\prime\)) je potem:
    \[
        \begin{array}{l r c r}
            min & (-b)^T y \\
            p.p. & (-A)^T y & \ge & -c \\
        \end{array}    
    \]
    
    \[
        \begin{array}{l r c r}
            max & b^T y \\
            p.p. & A^T y & \le & c \\
        \end{array}    
    \]

    \begin{theorem}
        Naj bo \(x\) \textit{ddr} in \(y\) cene razvoza ob koncu simpleksne metode na omrežjih. Potem je \(x\) optimalna rešitev za \(PR\), \(y\) pa za \(PR^\prime\).
    \end{theorem}


    \begin{proof}
        Ker je \(y_j - y_i \le c_{ij}\), je \(y\) dopustna rešitev za \(PR^\prime\). Ker je \(c^T x = b^T y\), je po ŠID \(x\) optimalna za \(PR\) in \(y\) optimalna za \(PR^\prime\).
    \end{proof}

    \begin{theorem}[Izrek o celoštevilskih rešitvah]
        Naj bo \(b_v \in \mathbb{Z}\) za vse povezave \(v \in V\). Potem velja:
        \begin{enumerate}[label=(\alph*)]
            \item Če ima problem dopustno rešitev, ima tudi celoštevilsko dopustno rešitev.
            \item Če ima problem optimalno rešitev, ima tudi celoštevilsko optimalno rešitev.
        \end{enumerate}
    \end{theorem}

    \begin{proof}
        Opravimo 1. fazo simpleksne metode na omrežjih in pridemo do dopustne rešitve. Prva \textit{ddr} je celoštevilska. Nadaljujemo z 2. fazo, in tu na vsakem koraku ostanemo v celih številih.
    \end{proof}


    \subsection{Dvojno stohastične matrike}


    \begin{definition}
        Matrika \(A \in \mathbb{R}^{n \times n}\) je \textbf{dvojno stohastična}, če so \(a_{ij} \ge 0\) in so vsote elementov vsake vrstice in stolpca enake 1.
    \end{definition}

    \begin{definition}
        Matrika \(A \in \mathbb{R}^{n \times n}\) je \textbf{permutacijska matrika}, če so \(a_{ij} \in \{0,1\}\) in je v vsaki vrstici in stolpcu natanko ena enica.
    \end{definition}

    Permutacijska matrika je očitno tudi dvojno stohastična.

    \begin{proposition}
        Naj bo \(A \in \mathbb{R}^{n \times n}\) dvojno stohastična matrika. Potem obstaja takšna permutacijska matrika \(P\), da velja \(p_{ij} > 0 \implies a_{ij} > 0\).
    \end{proposition}

    \begin{proof}
        Konstruiramo dvodelni graf \(G\), katerega vozlišča so vrstice in stolpci \(A\):
        \[
            \begin{array}{l l l}
                V & = & \{\ v_1,...,v_n,s_1,...,s_n\ \} \\
                E & = & \{\ v_i s_j\ |\ a_{ij} > 0\ \}
            \end{array}
        \]
        Nastavimo še uteži vozlišč in povezav \(b_{v_i}=-1\), \(b_{s_j}=1\), \(c_{v_i s_j}=0\). \\
        Na tem grafu Kirchhoffov zakon:
        \[
            \sum_{v \in V} b_v = -n + n = 0    
        \]
        Ta graf interpretiramo kot problem razvoza. Problem je dopusten, saj imamo rešitev \(x_{ij}=a_{ij}\):
        \[
            \sum_{K(e)=v_i} x_e - \sum_{Z(e)=v_i} x_e = 0 - a_{ij} = - \sum_{j:a_{ij}>0} a_{ij} = -1
        \]
        \[
            \sum_{K(e)=s_j} x_e - \sum_{Z(e)=s_j} x_e = a_{ij} - 0 = \sum_{i:a_{ij}>0} a_{ij} = 1
        \]
        Opomba: pri zadnji vsoti je pogoj \(a_{ij} > 0\), ker pri \(a_{ij}=0\) ni povezave v grafu, in zato tudi \(x_{ij}\) zanjo ne obstaja. \\
        Ker imamo dopustno rešitev \(a_{ij} \ge 0\), imamo tudi celoštevilsko dopustno rešitev \(p_{ij} \ge 0\). \\
        
    \end{proof}

    \begin{theorem}[Konigov izrek o plesnih parih]
        Naj bo \(k\)-regularen dvodelen graf \(G\). Potem ima \(G\) popolno prirejanje. 
    \end{theorem}
        
    \begin{proof}
        \(V(G)=X \cup Y\). Definiramo matriko \(A \in \mathbb{R}^{n \times n}\), z elementi:
        \[
            a_{ij}=
            \begin{cases}
                \frac{1}{k}: & x_i \sim y_j \\
                0: & x_i \nsim y_j    
            \end{cases}
        \]
        \(A\) je dvojno stohastična. Obstaja permutacijska matrika \(P\), za katero velja \(p_{ij} > 0 \implies a_{ij} > 0\). Torej, če je \(p_{ij} = 1\), je \(x_i \sim y_j\). Ker to velja pri vsakem \(x_i\) natanko za enega \(y_j\), in obratno, smo našli popolno prirejanje:
        \[
            M = \{\ x_i y_j \in E\ |\ p_{ij}=1\ \}    
        \]
    \end{proof}


    \subsection{Problem razvoza z omejitvami (PRO)}

    Problem razvoza z omejitvami je isti problem kot PR, le da imamo za vsako vozlišče \(e\) še omejitev \(u_e \in [0, \infty)\). Rešitev je dopustna, če \(Ax = b\) in \(0 \le x \le u\).

    \begin{definition}
        Naj bo \(x\) dopustna rešitev za PRO. \(x\) je drevesna dopustna rešitev, če obstaja takšno drevo \(T\), da za vsako \(e \notin T\) velja \(x_e = 0\) (\textbf{prazna povezava}) ali \(x_e = u_e\) (\textbf{nasičena povezava}).
    \end{definition}

    Uporabimo varianto simpleksne metode na omrežjih - tudi tu izberemo \(y_1 = 0\) in izračunamo \(y_j = y_i + c_{ij}\) za vse \(ij \in T\). Za vstopajočo povezavo lahko izberemo \(ij \notin T\), če velja \(x_{ij} = 0\) in \(y_i + c_{ij} < y_j\) ali \(x_{ij} = u_{ij}\) in \(y_i + c_{ij} > y_j\).
    \par
    Izbira izstopajoče povezave je odvisna od načina, na katerega smo izbrali vstopajočo povezavo.
    \begin{enumerate}[label=(\alph*)]
        \item Če za vstopajočo povezavo velja \(x_{ij}=0\) in \(y_i+c_{ij} < y_j\), potem za izstopajočo povezavo iščemo obratno povezavo, katero bomo zmanjšali na 0, ali pa premo povezavo, katero bomo povečali na \(u_e\). Ker mora veljati Kirchhoffov zakon in omejitev med 0 in \(u_e\) za vse povezave, bomo vse preme povezave povečali, obratne pa zmanjšali za takole definiran \(t\):
            \[
                t = \min \{\ x_e\ |\ e\ \text{obratna}\ \} \cup \{\ u_e - x_e\ |\ e\ \text{prema}\ \}
            \]
        \item Če pa za vstopajočo povezavo velja \(x_{ij}=u_e\) in \(y_i+c_{ij} > y_j\), pa preme povezave na ciklu zmanjšamo, obratne pa povečamo za \(t\):
            \[
                t = \min \{\ x_e\ |\ e\ \text{prema}\ \} \cup \{\ u_e - x_e\ |\ e\ \text{obratna}\ \}  
            \]
    \end{enumerate}

    \begin{theorem}
        Naj bo \(x\) \textit{ddr}. Če za vse \(ij \notin T\) velja:
        \begin{enumerate}[label=(\alph*)]
            \item \(x_{ij}=0 \implies y_i+c_{ij} \ge y_j\) ali
            \item \(x_{ij}=u_{ij} \implies y_i+c_{ij} \le y_j\)
        \end{enumerate}

        potem je \(x\) optimalna rešitev.
    \end{theorem}
        
    \begin{proof}
        \[
            \begin{array}{l r r}
                ij \in T: & y_i + c_{ij} = y_j & y_i + c_{ij} - y_j = 0 \\
                ij \notin T, x_e = 0: & y_i + c_{ij} \ge y_j & y_i + c_{ij} - y_j \ge 0 \\
                ij \notin T, x_e = u_e: & y_i + c_{ij} \le y_j & y_i + c_{ij} - y_j \le 0
            \end{array}    
        \]
        Naj bo \(\overline{x}\) dopustna rešitev. Pokažimo \((y_i + c_{ij} - y_j) \overline{x}_e \ge (y_i + c_{ij} - y_j) x_e\):
        \[
            \begin{array}{l l}
                ij \in T: & (y_i + c_{ij} - y_j) \overline{x}_e = (y_i + c_{ij} - y_j) x_e = 0 \\
                ij \notin T, x_e = 0: & (y_i + c_{ij} - y_j) \overline{x}_e \ge (y_i + c_{ij} - y_j) 0 = 0 \\
                ij \notin T, x_e = u_e: & (y_i + c_{ij} - y_j) u_e \le (y_i + c_{ij} - y_j) \overline{x}_e \le 0
            \end{array}    
        \]

        Seštejemo po vseh povezavah:
        \[
            c^T \overline{x} - (A^T y)^T \overline{x} = c^T \overline{x} - y^T A \overline{x} \ge c^T x - (A^T y)^T x = c^T x - y^T A x
        \]
        Ker sta \(x\) in \(\overline{x}\) dopustni rešitvi, za obe velja \(Ax=A \overline{x} = b\).
        \[
            c^T \overline{x} - y^T b \ge c^T x - y^T b     
        \]
        \[
            c^T \overline{x} \ge c^T x 
        \]
        Ker je poljubna dopustna rešitev večja ali enaka \(x\), je \(x\) optimalna.
    \end{proof}

    Če ne moremo najti izstopajoče povezave, so vse povezave v ciklu preme in \(u_e=\infty\). Podobno kot pri navadnem PR imamo takrat negativen cikel in problem je neomejen. Podobno se postopek zanesljivo ustavi, če uporabimmo Cunninghamovo pravilo (tj. če imamo več kandidatov za izstoopno povezavo, izberemo prvo na poti od korena do vstopne spremenljivke).
    \par
    Začetno \textit{ddr} poiščemo na skoraj enak način kot pri navadnem PR: umetnim povezavam damo omejitev \(u_e=\infty\).

    \begin{remark}
        Pri PRO se lahko zgodi, da je neka povezava vstopna in izstopna naenkrat.
    \end{remark}


    \pagebreak
    \section{Pretoki in prerezi}

    Naj bo \(G = (V,E\) usmerjen graf. Vsaki povezavi \(e\) smo dali utež \(u_e \in \mathbb{R}^+ \cup \infty\). Eno izmed vozlišč smo označili s \(s\) (kot \"source\"), neko drugo vozlišče pa s \(t\) (kot \"terminal\"). Velja pravilo, da \(s\) ni končno vozlišče nobene povezave, \(t\) pa ni začetno vozlišče nobene povezave.
    \par
    Iščemo pretok \((x_e)_{e \in E(G)}\) (angl. flow).
    \[
        0 \le x_e \le u_e\ \forall e \in E(G)    
    \] 
    Kirchhoffov zakon velja za vsa vozlišča \(v \in V \backslash \{s,t\}\):
    \[
        \sum_{K(e)=v} x_e = \sum_{Z(e)=v} x_e   
    \]

    Iščemo pretok, za katerega je \textbf{prostornina} največja:
    \[
        v = \sum_{Z(s)} x_e    
    \]

    Poglejmo si še enkrat Kirchhoffov zakon:
    \[
        \sum_{K(e)=v} x_e = \sum_{Z(e)=v} x_e  
    \]
    Seštejmo po vseh vozliščih razen \(s\) in \(t\).
    \[
        \sum_{K(e) \neq t} x_e = \sum_{Z(e) \neq s} x_e    
    \]
    \[
        \sum_{Z(e)=s} x_e = \sum_{K(e)=t} x_e = v
    \]

    Problem se prevede na PRO:
    \[
        b_v=0\ \text{za}\ \forall v \in V    
    \]
    dodamo povezavo \(ts\): \(u_{ts}=\infty\), \(c_{ts} = -1\), \(c_e=0\ \forall e \in E\).
    \par
    Problem lahko rešimo preko simpleksne metode na omrežjih, vendar je bolj primeren \textbf{Ford-Fulkersonov algoritem}.

    \begin{definition}
        Prerez (angl. cut) je podmnožica \(C \subset V(G)\), za katero je \(s \in C\) in \(t \notin C\).
    \end{definition}

    Znova si oglejmo posledice Kirchhoffovega zakona:
    \[
        \sum_{Z(e)=v} x_e = \sum_{K(e)=v} x_e  
    \]

    To velja za vsa vozlišča v \(C \backslash \{s\}\), zato seštejemo po vseh teh vozliščih (nobena povezava se ne začne v \(t\) ali konča v \(s\)):
    \[
        \sum_{Z(e) \in C \backslash \{s\}} x_e = \sum_{K(e) \in C} x_e    
    \]
    Na obeh straneh enačbe odštejemo povezave, ki se začnejo in končajo v \(C \backslash \{s\}\).
    \[
        \sum_{\substack{Z(e) \in C \backslash \{s\} \\ K(e) \notin C}} x_e = \sum_{\substack{Z(e) \notin C \backslash \{s\} \\ K(e) \in C}} x_e    
    \]
    Oboje premaknemo na eno stran:
    \[
         \sum_{\substack{Z(e) \in C \backslash \{s\} \\ K(e) \notin C}} x_e\ - \sum_{\substack{Z(e) \notin C \backslash \{s\} \\ K(e) \in C \backslash \{s\}}} x_e\ = 0 
    \]
    Prištejemo \(\displaystyle v = \sum_{Z(e)=s} x_e\):
    \[
        v = \sum_{\substack{Z(e) \in C \backslash \{s\} \\ K(e) \notin C}} x_e\ - \sum_{\substack{Z(e) \notin C \backslash \{s\} \\ K(e) \in C}} x_e\ + \sum_{Z(e)=s} x_e
    \]
    \[
        v = \sum_{\substack{Z(e) \in C \backslash \{s\} \\ K(e) \notin C}} x_e\ - \sum_{\substack{Z(e) \notin C \backslash \{s\} \\ K(e) \in C}} x_e\ + \sum_{\substack{Z(e)=s\\ K(e) \notin C}} x_e + \sum_{\substack{Z(e)=s\\ K(e) \in C}} x_e
    \]
    \[
        v = \sum_{\substack{Z(e) \in C \\ K(e) \notin C}} x_e\ - \sum_{\substack{Z(e) \notin C \backslash \{s\} \\ K(e) \in C}} x_e\ + \sum_{\substack{Z(e)=s\\ K(e) \in C}} x_e
    \]
    \[
        v = \sum_{\substack{Z(e) \in C \\ K(e) \notin C}} x_e\ - \sum_{\substack{Z(e) \notin C \\ K(e) \in C}} x_e\ - \sum_{\substack{Z(e)=s \\ K(e) \in C}} x_e\ + \sum_{\substack{Z(e)=s \\ K(e) \in C}} x_e 
    \]

    \[
        v = \sum_{\substack{Z(e) \in C \\ K(e) \notin C}} x_e\ - \sum_{\substack{Z(e) \notin C \\ K(e) \in C}} x_e\
    \]
    \[
        v = \sum_{\substack{i \in C \\ j \notin C}} x_{ij}\ -\ \sum_{\substack{i \notin C \\ j \in C}} x_{ij}\
    \]

    Prostornina pretoka je torej razlika med pretokom, ki odhaja iz \(C\) in pretokom, ki pride v \(C\).

    \begin{definition}
        Vsota omejitev pretoka po povezavah, ki se začnejo v prerezu \(C\) in se končajo izven \(C\), imenujemo \textbf{kapaciteta prereza} \(C\).
        \[
            \sum_{\substack{i \in C \\ j \notin C}} u_{ij} \in \mathbb{R}^+ \cup \infty
        \]
    \end{definition}

    Iz pokazanega sledi naslednji izrek.
    \begin{theorem}
        Naj bo \(x\) poljubni pretok in \(C\) poljuben prerez omrežja. Potem je prostornina pretoka \(x\) manjša ali kvečjemu enaka kot kapaciteta prereza \(C\). Če sta enaki, je \(x\) maksimalen pretok in \(C\) minimalen prerez.
    \end{theorem}

    Temu problemu pravimo \textbf{problem minimalnega prereza (min cut)}. Naslednji izrek pove, da v bistvu problemu lahko rečemo \textbf{max flow min cut}.

    \begin{theorem}
        Velja natanko ena od možnosti:
        \begin{enumerate}[label=(\alph*)]
            \item Obstajajo pretoki s poljubno veliko prostornino. Potem imajo vsi prerezi kapaciteto \(\infty\).
            \item Obstaja max pretok. Njegova prostornina je enaka kapaciteti min prereza.
        \end{enumerate}
    \end{theorem}

    \begin{proof}
        Dokazali bomo preko PRO. Ponovimo: \(u_{ts}=\infty, c_{ts}=-1, c_e=0\ \forall e \in E\). Problem je dopusten, saj je rešitev \(x_e=0\) očitno dopustna.
        \par
        Če je problem neomejen, je lahko prostornina poljubno velika. Ker je kapaciteta poljubnega prereza večja od prostornine, mora biti neskončna za vse prereze.
        \par
        Če je problem omejen, obstaja maksimalen pretok \(x\). Vemo, da obstaja povezava \(ts\), in zanjo velja \(u_{ts}=\infty\). Povezava \(ts\) zagotovo ni nasičena, torej je \(y_t + c_{ts} = y_t - 1 \ge y_s\) (Če je \(ts\) v drevesu optimalne rešitve, imamo enakost. Sicer je \(x_{ts}=0\) in lahko velja tudi neenakost). Definiramo množico:
        \[C = \{\ v \in V\ |\ y_v \le y_s \}\]
        Najprej pokažimo, da je ta množica prerez. \(y_s \le y_s\), torej je \(s \in C\). \(y_t \ge y_s + 1\), torej \(y_t > y_s\), zato \(t \notin C\). \(C\) je torej prerez. Sedaj moramo še pokazati, da sta prostornina pretoka in kapaciteta prereza enaki.
        \par
        Prostornina maksimalnega pretoka je
        \[
            v = \sum_{\substack{i \in C \\ j \notin C}} x_{ij}\ - \sum_{\substack{i \notin C \\ j \in C}} x_{ij} 
        \]
        Vzemimo takšno povezavo \(ij \in E\), da je \(i \in C\) in \(j \notin C\). Potem je \(y_i \le y_s < y_j\). Ker je \(y_i + c_{ij} = y_i + 0 < y_j\), je povezava \(ij\) zasičena, torej je \(x_{ij} = u_{ij}\).
        \par
        Vzemimo sedaj še povezavo \(ij \in E\), za katero velja \(i \notin C\) in \(j \in C\). Potem je \(y_i > y_s \ge y_j\). Podobno velja \(y_i > y_j\) in je povezava \(ij\) prazna, torej \(x_{ij} = 0\). Iz obeh opažanj sledi
        \[
            v = \sum_{\substack{i \in C \\ j \notin C}} x_{ij}\ - \sum_{\substack{i \notin C \\ j \in C}} x_{ij} = v = \sum_{\substack{i \in C \\ j \notin C}} u_{ij}\ - \sum_{\substack{i \notin C \\ j \in C}} 0 = v = \sum_{\substack{i \in C \\ j \notin C}} u_{ij}
        \]
        To pa je ravno kapaciteta prereza \(C\). Ker sta prostornina pretoka in kapaciteta prereza enaki, je \(x\) maksimalni pretok in \(C\) minimalni prerez. 
    \end{proof}


    \subsection{Ford-Fulkersonov algoritem}

    Začnemo s praznim pretokom (\(x_e =  0\) za vse \(e \in E\)).

    \begin{definition}
        \textbf{Povečujoča pot} (angl. augmenting path) je pot \(v_0 v_1 v_2 ... v_l\), za katero je \(v_0 = s\) in \(v_l = t\) ter za vse primerne \(i\) velja:
        \[
            \begin{array}{l l}
                v_i v_{i+1} \in E(G): & x_{v_i v_{i+1}} < u_{v_i v_{i+1}} \\
                \text{ali} \\
                v_{i+1} v_i \in E(G): & x_{v_{i+1} v_i} > 0
            \end{array}
        \]
        tj., preme povezave niso zasičene in obratne povezave niso prazne.
    \end{definition}

    Po povečujoči poti lahko pretok povečamo toliko, da bomo zasičili oz. izpraznili vsaj eno premo oz. obratno povezavo.
    \[
        t = \min \{\ u_e - x_e\ |\ e\ \text{prema}\ \} \cup \{\ x_e\ |\ e\ \text{obratna}\ \}
    \]
    Kirchhoffov zakon še vedno velja v vsakem vozlišču: če obe povezavi kažeta v isto smer, se je pretok na obeh povečal za \(t\), , če pa ena kaže v premo, ena pa v obratno smer, pa se je eni povečal, drugi pa zmanjšal pretok za \(t\), zatp je razlika tudi v tem primeru še vedno enaka \(0\).
    \par
    Ker smo pretok povečali za minimum možnih povečanj, nismo nobene povezave zmanjšali pod 0 ali nad \(u_e\), zato je pretok po povečanju še vedno dopusten in ima za \(t\) večjo prostornino. Največji pretok smo našli, ko ni več nobene povečujoče poti.
    \par
    Povečujočo pot iščemo na takšen način, da hkrati sestavljamo prerez: začnemo z množico \(C=\{s\}\) in dodamo preme sosede, do katerih pridemo preko nezasičene povezave ter obratne sosede, do katerih pridemo preko neprazne povezave. Iz vsakega dodanega vozlišča na isti način dodajamo njihove sosede in se ustavimo tedaj, ko dodamo \(t\), ali pa smo pregledali vsa vozlišča. Če je \(t \in C\), smo našli povečujočo pot. V tem primeru povečamo povezave na poti in ponovimo iskanje poti. Če pa je \(t \notin C\) in ne moremo dodati nobenega vozlišča, pa smo našli prerez \(C\) in končamo. Vse preme povezave, ki vodijo izven \(C\), so zagotovo zasičene, vse obratne povezave, ki vodijo v \(C\), pa so zagotovo prazne. Torej velja:
    \[
        v = \sum_{\substack{i \in C \\ j \notin C}} x_{ij}\ -\ \sum_{\substack{i \notin C \\ j \in C}} x_{ij} = \sum_{\substack{i \in C \\ j \notin C}} u_{ij}
    \]
    \(C\) ima kapaciteto, enako prostornini pretoka (ker so vse preme povezave s koncem v \(t\) zasičene, obratne pa prazne), torej je \(x\) maksimalni pretok in \(C\) minimalni prerez.

    \begin{remark}
        Ford-Fulkersonov algoritem se ne konča vedno.
    \end{remark}



    \pagebreak
    \section{Prirejanja in pokritja}

    \begin{definition}
        Naj bo \(G\) (neusmerjen) graf. \(M \subseteq E(G)\) je \textbf{prirejanje} (angl. matching), če je \(e \cap f = \emptyset\) za vse \(e,f \in M\), \(e \neq f\) (torej množica povezav, ki nimajo skupnih vozlišč). \(P \subseteq V(G)\) je \textbf{pokritje} (angl. cover), če za vse \(e \in E(G)\) obstaja takšen \(v \in P\), da je \(v \in e\) oz. \(e \cap P \neq \emptyset\) (torej množica vozlišč, ki vsebuje vsaj eno vozlišče vsake povezave).
    \end{definition}

    Iščemo največje prirejanje (velikost označimo \(\mu(G)\)) in najmanjše pokritje (velikost označimo \(\tau(G)\)).

    \begin{proposition}
        Naj bo \(G\) graf. Potem velja
        \begin{enumerate}[label=(\alph*)]
            \item Za vsako prirejanje \(M\) in pokritje \(P\) velja \(|M| \le |P|\).
            \item Če za prirejanje \(M\) in pokritje \(P\) velja \(|M|=|P|\), potem je \(\mu(G) = \tau(G) = |M| = |P|\).
            \item Za vsako prirejanje \(M\) in pokritje \(P\) velja \(\mu(G) \le \tau(G)\).
        \end{enumerate}
    \end{proposition}

    \begin{proof}[Dokaz \emph{(a)}]
        Obravnavamo preslikavo \(f\): \(M \to P\), definirano kot \(e \mapsto\) vozlišče v \(P \cap e\). Ker je \(P\) pokritje, je ta preslikava definirana za vse \(e \in M\). Ker je \(M\) prirejanje, je \(f\) injektivna (saj nimata nobeni dve vozlišči skupnega vozlišča), torej je \(|M| \le |P|\).
    \end{proof}

    \begin{proof}[Dokaz \emph{(b)}]
        Če velja \(|M|=|P|\), potem \(M\) ne more biti večji, \(P\) pa ne more biti manjši. Zato je res \(\mu(G) = \tau(G) = |M| = |P|\)
    \end{proof}

    \begin{proof}[Dokaz \emph{(c)}]
        Ker velja \(|M| \le |P|\) za vsa prirejanja \(M\) in pokritja \(P\), to velja tudi za velikosti največjega prirejanja in najmanjšega pokritja. 
    \end{proof}

    \begin{remark}
        V splošnem ni \(\mu(G) = \tau(G)\). Videli bomo, da to velja za dvodelne grafe.
    \end{remark}

    Kako pa poiščemo največje prirejanje? \\
    
    Naj bo \(M\) prirejanje grafa \(G\). Povezavi \(e \in M\) rečemo \textbf{vezana}, povezavi \(e \notin M\) pa \textbf{prosta}. Vozlišče \(v \in V(G)\) je vezano, če je \(v \cap e \neq \emptyset\) za nek \(e \in M\), tj. \(v\) je vozlišče neke povezave v \(M\). Vozlišče \(v \in V(G)\) je prosto, če je \(v \cap e = \emptyset\) za vse \(e \in M\), tj. \(v\) ni vozliŠče nobene povezave v \(M\).
    
    \begin{definition}
        Pot v grafu \(G\) je \textbf{izmenična/alternirajoča} glede na prirejanje \(M\), če se na njej izmenjujejo proste in vezane povezave.
    \end{definition}

    \begin{definition}
        Alterirajoča pot je \textbf{povečujoča}, če se začne in konča s prostim vozliščem.
    \end{definition}

    Spomnimo se operacije Boolove vsote oz. simetrične razlike. Boolova vsota dveh množic \(A\) in \(B\) je množica, ki vsebuje elemente \(A\) in elemente \(B\), ki se ne pojavijo v obeh.

    \begin{proposition}
        Naj bo \(M\) prirejanje in \(P\) povečujoča pot na grafu \(G\). Potem je \(M^\prime = M \oplus E(P)\) prirejanje in \(|M^\prime| = |M|+1\). 
    \end{proposition}

    \begin{proof}
        Vezana vozlišča na povečujoči (torej tudi alternirajoči) poti \(P\) so elementi \(M \cap E(P)\) in zato niso elementi \(M^\prime\). Ker je \(P\) povečujoča pot, se začne in konča s prostim vozliščem, torej se začne in konča s prosto povezavo. Ker je alternirajoča, je število prostih povezav za 1 večje od števila vezanih povezav, torej je \(|M^\prime| = |M|+1\). Povezave v \(M^\prime\) tvorijo enako množico vozlišč kot povezave v \(M\). Ker povezave v \(M\) nimajo skupnih vozlišč, jih tudi povezave v \(M^\prime\) nimajo. \(M^\prime\) je torej prirejanje.
    \end{proof}

    \begin{theorem}[Bergeov izrek]
        Prirejanje \(M\) na grafu \(G\) je največje prirejanje natako tedaj, ko na \(M\) ne obstaja nobena povečujoča pot.
    \end{theorem}

    \begin{proof}[Dokaz \(\implies\)]
        Če povečujoča pot ne obstaja, potem po prejšnji trditvi ne obstaja prirejanje \(M^\prime\), za katero bi veljalo \(|M^\prime| = |M|+1\). Potem tudi ne obstaja nobeno prirejanje še večje velikosti, ker bi v tem primeru dobili \(M^\prime\) tako, da bi  odstranili poljubne povezave.
    \end{proof}

    \begin{proof}[Dokaz \(\Longleftarrow\)]
        Recimo, da obstaja prirejanje \(M^\prime\), ki je večje od nekega prirejanja \(M\). Konstruirajmo graf \(H\), za katerega velja \(V(H)=V(G)\) in \(E(H)=M \oplus M^\prime\). Poglejmo povezane komponente grafa \(H\): med njimi so poti, ki so alternirajoče, saj si dve povezavi v prirejanju nikoli ne delita povezave; druge komponente so pa alternirajoči, in zato sodi cikli. V teh ciklih se pojavi enako število povezav iz \(M\) in \(M^\prime\). Ker pa je \(|M^\prime| > |M|\), mora obstajati vsaj ena pot, ki ima več povezav iz \(M^\prime\) kot \(M\). Takšna pot se začne in konča s povezavo iz \(M^\prime\). Vozlišči na začetku in koncu takšne poti sta prosti glede na \(M\): sicer bi takšno vozlišče bile tako v povezavi iz \(M\) kot v povezavi iz \(M^\prime\), ampak v \(M^\prime\) ne more biti, ker bi si potem dve povezavi iz \(M^\prime\) delili povezavo, a potem \(M^\prime\) ne bi bilo prirejanje. Torej smo našli povečujočo pot glede na \(M\), kar pa je v protislovju s predpostavko, da povečujoča pot ne obstaja.
    \end{proof}

    Bergeov izrek prevede problem iskanja največjega prirejanja na problem iskanja povečujoče poti. V splošnih grafih ga lahko iščemo s Edmondsovim algoritmom (angl. blossom algorithm), v dvodelnih pa uporabimo \textbf{madžarsko metodo}.



    \subsection{Madžarska metoda}
    Naj bo \(G\) dvodelen graf, torej \(V(G)=X \cup Y\), \(X \cap Y = \emptyset\). \\
    Vzemimo poljubno prirejanje \(M\). Nastavimo \(S:=\) prosta vozlišča v \(X\) in \(T:= \emptyset\). Ponavljamo dodajanje vozlišč v \(S\) in \(T\):
    \[
        \begin{array}{ l l }
            S:= & \text{stari } S \cup \{\ \text{vozlišča v}\ X,\ \text{do katerih pridemo po vezanih povezavah v}\ T\ \} \\

            T:= & \text{stari } T \cup \{\ \text{vozlišča v}\ Y,\ \text{do katerih pridemo po prostih povezavah v}\ S\ \}
        \end{array}
    \]

    Ponavljamo, dokler nimamo prostega vozlišča v \(T\), ali pa v zadnji iteraciji nismo dodali nobenega vozlišča (torej \(S = \text{stari}\ S\) in \(T = \text{stari}\ T\)). Če je v \(T\) prosto vozlišče, smo našli povečujočo pot, ki se začne v enem izmed prostih vozlišč v \(X\) (ki je tudi v \(S\)) in konča s prostim vozliščem v \(T\). To pot smo zagotovo našli, ker smo na vsakem koraku dodajanja vozlišč uporabili povezavo, ki po neki poti izhaja iz prostih vozlišč v \(X\). Postopek ponovimo na prirejanju \(M^\prime = M \oplus E(P)\).
    \par
    Če v \(T\) ni prostega vozlišča, postopek zaključimo in vrnemo prirejanje \(M\). Našli smo tudi pokritje \(P=(X \backslash S) \cup T\).

    \begin{theorem}
        Naj bo \(G\) dvodelen graf. Ob zaključku madžarske metode je \(M\) največje prirejanje in \(P=(X \backslash S) \cup T\) najmanjše pokritje. Velja \(|M|=|P|=\mu(G)=\tau(G)\).
    \end{theorem}

    \begin{proof}
        Vozlišča razdelimo na štiri podmnožice: \(S\), \(T\), \(X \backslash S\) in \(Y \backslash T\). Med \(S\) in \(Y \backslash T\) ni nobene povezave: če bi bla prosta, bi lahko opravili še eden korak madžarske metode in bi povečali \(T\); ne more biti vezana, ker je vsako vozlišče v \(S\) prosto, ali pa ima povezavo z vozliščem v \(T\) (povezave z \(Y \backslash T\) nima, ker smo ga dodali v \(S\) zaradi vezane povezave, ki je bila v koraku prej že dodana v \(T\)). Med \(X \backslash S\) in \(T\) pa ni vezanih povezav: če bi bila, bi lahko opravili še en korak in povečali \(S\). 
        \par
        Definiramo množici
        \[
            \begin{array}{l l l}
                M_1 & = & \{\ e \in M\ |\ e\ \text{povezava med}\ S\ \text{in}\ T\ \} \\
                M_2 & = & \{\ e \in M\ |\ e\ \text{povezava med}\ X \backslash S\ \text{in}\ Y \backslash T\ \}
            \end{array}
        \]
        Ker med \(X \backslash S\) in \(T\) ni nobene povezave ter med \(S\) in \(Y \backslash T\) ni nobene vezane povezave, je
        \[
            M = M_1 \cup M_2
        \]
        in ker sta \(M_1\) in \(M_2\) disjunktni, je \(|M|=|M_1|+|M_2|\). Vsa vozlišča v \(T\) so vezana in za vsako je natanko ena povezava vezana, zato je \(|M_1|=|T|\). Podobno, vozlišča v \(X \backslash S\) so tudi vsa vezana, ker smo na začetku v \(S\) vstavili vsa prosta vozlišča v \(X\), zato je \(|M_2|=|X \backslash S|\). Torej je \(|M|=|T| + |X \backslash S\).
        \par
        Poglejmo si \(P=(X \backslash S) \cup T\). Vidimo, da je \(|P|=|X \backslash S| + |T| = |M|\). \(P\) je pokritje, ker med \(S\) in \(Y \backslash T\) ni nobene povezave, povezave med \(S\) in \(T\) ter \(X \backslash S\) in \(Y \backslash T\) imajo po eno vozlišče v \(P\), povezave med \(X \backslash S\) in \(T\) pa imajo obe vozlišči v \(P\). Ker je \(|M|=|P|\), je \(M\) največje prirejanje in \(P\) najmanjše pokritje.
    \end{proof}

    \begin{remark}
        Za dvodelni graf \(G\) velja \(\mu(G)=\tau(G)\).
    \end{remark}

    \begin{definition}
        Prirejanje je \textbf{popolno}, če so vsa vozlišča grafa vezana.
    \end{definition}

    Graf z lihim številom vozlišč nima popolnega prirejanja, saj vsaka povezava v prirejanju veže dve vozlišči in noben par dveh povezav v prirejanju si ne deli vozlišča.

    \begin{remark}
        Konigov izrek o plesnih parih govori o \(k\)-regularnih dvodelnih grafih s popolnim prirejanjem.
    \end{remark}

    \begin{theorem}[Hallov izrek]
        V dvodelnem grafu \(G=X \cup Y\) obstaja popolno prirejanje iz \(X\) v \(Y\) (vsa vozlišča v \(X\) so vezana) natanko tedaj, ko za vse \(A \subseteq X\) velja \(|A| \le |N(A)|\).
    \end{theorem}

    \begin{proof}[Dokaz \(\implies\)]
        Naj za dvodelen graf \(G\) obstaja popolno prirejanje \(M\). Za vsako \(A \subseteq x\) Definiramo preslikavo \(\phi\), ki vsako vozlišče \(x \in X\) slika v vozlišče \(y \in N(A) \subseteq Y\), če je povezava \(xy \in M\). Ker je \(M\) popolno prirejanje, je \(\phi\) definirana za vse \(x \in X\), hkrati pa je injektivna, ker povezava \(xz \notin M\), če je \(xy \in M\).
    \end{proof}

    \begin{proof}[Dokaz \(\Longleftarrow\)]
        Na dvodelnem grafu \(G\) uporabimo madžarsko metodo. Vozlišča se razdelijo v množice \(S\), \(T\), \(X \backslash S\) in \(Y \backslash T\), dobimo največje prirejanje \(M\) in najmanjše pokritje \(P\).
        \par
        \(S \subseteq X\), zato zanjo velja \(|S| \le |N(S)|\). Ampak vsa vozlišča, ki so sosednja za \(S\), so v \(T\), torej je \(|S| \le |N(S)| \le |T|\). Po končani madžarski metodi je \(|M|=|P|=|X \backslash S| + |T| \ge |X \backslash S| + |S| = |X|\). \(|M| \le |X|\), zato je \(|M| = |X|\), torej je \(M\) popolno prirejanje.
    \end{proof}



    \subsection{Minimalno popolno prirejanje (madžarska metoda z utežmi)}

    Imamo polni dvodelni graf \(K_{n,n}\). Vsaka povezava ima utež v \(\mathbb{R} \cup \{- \infty, \infty \}\). \\
    Uteži izrazimo z matriko \(C = [c_{ij}]_{i,j=1}^n\). Zanima nas popolno prirejanje \(M\) z \(\min (\sum_{i,j \in M} c_{ij})\).

    \begin{remark}
        Ta problem se pogosto interpretira kot problem razvrščanja (angl. assignment problem).
    \end{remark}

    \begin{remark}
        Če rešujemo problem maksimalnega popolnega prirejanja, rešujemo problem minimalnega popolnega prirejanja za \(-C\).
    \end{remark}
    
    \begin{remark}
        Iščemo permutacijo \(\pi \in S_n\), da je \(\sum_{i=1}^n c_{i \pi(i)}\) minimalna. Če bi iskali ciklično permutacijo, imamo problem potujočega trgovca (angl. travelling salesman).
    \end{remark}

    \begin{proposition}
        Problem se ne spremeni, če vrstici ali stolpcu matrike \(C\) prištejemo neko število.
    \end{proposition}

    \begin{proof}
        Če neko število prištejemo vrstici \(i\), smo to vrednost prišteli vsem povezavam, ki imajo na levi krajišče \(i\). Vsako popolno prirejanje vsebuje natanko eno od teh povezav. Analogno razmislimo za stolpce.
    \end{proof}

    Koraki madžarske metode z utežmi (MMU):
    \begin{enumerate}[label=\arabic*. korak]
        \item Od vsake vrstice odštejemo najmanjšo vrednost. Od vsakega stolpca odštejemo najmanjšo vrednost. Sedaj so vse uteži \(\ge 0\), v vsaki vrstici in stolpcu je vsaj ena ničla.
        \item Pogledamo, če obstaja \(n\) ničel, in da je v vsaki vrstici in stolpcu natanko ena. V tem primeru smo našli minimalno popolno prirejanje. Sicer lahko vse ničle pokrijemo z manj kot \(n\) vrsticami in stolpci.
            \par
            \(\epsilon := \) najmanjše nepokrito pozitivno število. Od nepokritih števil odštejemo \(\epsilon\), dvakrat pokritim številom pa prištejemo \(\epsilon\). Ta korak ponavljamo, dokler ne dobimo \(n\) ničel tako, da je ena v vsaki vrstici in stolpcu.
    \end{enumerate}

    2. korak izvedemo z navadno madžarsko metodo (MM), in sicer iščemo največje prirejanje na t.i. "ničelnem grafu". \\
    Konstruirajmo ničelni graf. To je graf \(H \subseteq K_{n,n}\), za katerega je 
    \[
        \begin{array}{ l l l }
            V(H) & = & \{\ v_1,...,v_n,s_1,...,s_n\ \} \\
            E(H) & = & \{\ v_i s_j\ |\ c_{ij}=0\ \}
        \end{array}  
    \]
    Vozlišča \(v_i\) predstavljajo vrstice, vozlišča \(s_j\) pa stolpce. Povezave predstavljajo ničelne uteži. \\
    Na koncu MM se lahko zgodi dvoje:
    \begin{itemize}
        \item \(M\) je popolno prirejanje oz. \(|M|=n\). \(M\) je tudi minimalno popolno prirejanje.
        \item \(M\) ni popolno prirejanje oz. \(|P| = |M| < n\). Ker je \(P\) pokritje, smo našli manj kot \(n\) vrstic in stolpcev, ki vsebujejo vse ničle.
    \end{itemize}

    Vsem vrsticam v \(P\) (torej v \(X \backslash S\)) prištejemo \(\epsilon\), vsem stolpcem, ki niso v \(P\) (torej v \(Y \backslash T\)) odštejemo \(\epsilon\) (v \(P\) so stolpci v \(T\) in vrstice v \(X \backslash S\)). \\
    Zakaj se postopek ustavi? \\
    Povezave med \(X \backslash S\) in \(T\) izginejo, ker smo povezavam s krajiščem v \(X \backslash S\) prišteli \(\epsilon\), torej za te povezavo ni več \(c_{ij} = 0\). Se je pa pojavila vsaj ena povezava med \(S\) in \(Y \backslash T\), ker smo stolpcem v \(Y \backslash T\) odšteli \(\epsilon\), ki je definiran kot najmanjše nepokrito število - nepokrita števila so pa ravno uteži povezav z vozliščema v \(Y \backslash T\) in \(S\). V povezavi \(xy\) (\(x \in S\), \(y \in Y \backslash T\)) je nastala ničla \(c_{xy}=0\).
    \par
    \(M\) je še vedno prirejanje v novem grafu. Na tem novem grafu in prirejanju \(M\) znova izvedemo MM: v nekem koraku dodamo \(x\) V \(S\), in takoj zatem še \(y\) v \(T\), ker do \(y\) pridemo iz \(x\) po prosti povezavi. 
    \[
        \begin{array}{ l l }
            y\ \text{prosto vozlišče:} & \text{našli smo povečujočo pot.}\ M\ \text{se poveča.} \\
            y\ \text{vezano vozlišče:} & \text{nadaljujemo MM.}\ S\ \text{se je povečal.}
        \end{array}    
    \]
    \(S\) se lahko poveča največ \(n\)-krat, preden zmanjka povezav v \(X\). MMU se torej konča v največ \(n(n+1)\) korakih.



    \pagebreak
    \section{Konveksna optimizacija}

    \begin{definition}
        Množica \(K \subseteq \mathbb{R}^n\) je \textbf{konveksna}, če je zveznica med poljubnima točkama v \(K\) celotna v \(K\).
    \end{definition}

    \begin{definition}
        Množica \(K \subseteq \mathbb{R}^n\) je \textbf{konveksna}, če za vse \(x,y \in K\) in \(\lambda \in [0,1]\) velja:
        \[
            (1-\lambda)x + \lambda y \in K    
        \]
        Ekvivalentno, za vse \(\lambda, \mu \ge 0\), \(\lambda + \mu = 1\) velja \(\lambda x + \mu y \in K\).
    \end{definition}

    \begin{proposition}
        Poljuben presek konveksnih množic je konveksna množica.
    \end{proposition}

    \begin{proof}
        Imamo konveksne množice \(A_i\). \\
        Vzamemo \(x,y \in \bigcap A_i\) in \(\lambda \in [0,1]\). Za vsak \(A_i\) velja \((1-\lambda) x + \lambda y \in A_i\). 
        Ker to velja za vse \(A_i\), velja tudi za \(\bigcap A_i\), torej \((1-\lambda) x + \lambda y \in \bigcap A_i\).
    \end{proof}

    \begin{definition}
        \textbf{Konveksna kombinacija} vektorjev \(x_1,...x_k\) je \(\lambda_1 x_1 + ... + \lambda_k x_k\) za \(\lambda_1,...,\lambda_k \ge 0\), \(\lambda_1 + ... + \lambda_k=1\).
    \end{definition}

    \begin{proposition}
        V konveksni množici \(K\) je vsaka konveksna kombinacije vektorjev iz \(K\) tudi v \(K\).
    \end{proposition}

    \begin{proof}
        Dokažemo z indukcijo na številu vektorjev v konveksni kombinaciji. Za \(k=1\) in \(k=2\) vemo iz definicije konveksnosti. Predpostavimo, da je konveksna kombinacija \(k-1\) vektorjev v \(K\) tudi v \(K\). \\
        Vzamemo poljubno konveksno kombinacijo \(\lambda_1 x_1 + ... + \lambda_k x_k\). Če je \(\lambda_k = 1\), so druge \(\lambda_i = 0\), zato je \(\lambda_1 x_1 + ... + \lambda_k x_k = x_k \in K\). Preverimo še primer, ko je \(\lambda_k < 1\). \(\lambda_1 x_1 + ... + \lambda_{k-1} x_{k-1}\) ni konveksna kombinacija, saj je \(\lambda_1 + ... + \lambda_{k-1} \le 1\), vendar to lahko popravimo. Velja \(\lambda_1 + ... + \lambda_{k-1} = 1 - \lambda_k\). Delimo celotno enačbo z \(1-\lambda_k\) in dobimo \(\frac{\lambda_1}{1-\lambda_k} + ... + \frac{\lambda_{k-1}}{1-\lambda_k} = \frac{\lambda_k}{1-\lambda_k}=1\). Torej je \(\frac{\lambda_1}{1-\lambda_k} x_1 + ... + \frac{\lambda_1}{1-\lambda_k} x_{k-1}\) konveksna kombinacija in po indukcijski predpostavki \(\frac{\lambda_1}{1-\lambda_k} x_1 + ... + \frac{\lambda_1}{1-\lambda_k} x_{k-1} \in K\). Prvotna kombinacija je \((1-\lambda_k) (\frac{\lambda_1}{1-\lambda_k} x_1 + ... + \frac{\lambda_k}{1-\lambda_k} x_{k-1}) + \lambda_k x_k \in K\).
    \end{proof}

    \begin{definition}
        \textbf{Konveksna ogrinjača} množice vektorjev \(A \mathbb{R}^n\) je
        \[
            C(A)=\bigcap_{\substack{K\ \text{konv.} \\ A \subseteq K}} K
        \]
    \end{definition}

    \begin{proposition}[Karakterizacija konveksnih ogrinjač]
        \(\) \\
        \begin{enumerate}[label=(\arabic*)]
            \item \(A \subseteq C(A)\)
            \item \(C(A)\) je konveksna.
            \item \(A \subseteq B^{\text{konv.}} \implies C(A) \subseteq B\)
            \item \(A^{\text{konv.}} \implies C(A)=A\)
            \item \(C(A) = \{\ \text{konveksne kombinacije točk iz}\ A\ \}\) 
        \end{enumerate}
    \end{proposition}
        
    \begin{proof}[Dokaz \emph{(1)}]
        Presek množic, ki vsebujejo \(A\), vsebuje \(A\). Zato je 
        \[
            C(A)=\bigcap_{\substack{K\ \text{konv.} \\ A \subseteq K}} K \supseteq A  
        \]
    \end{proof}

    \begin{proof}[Dokaz \emph{(2)}]
        \(C(A)\) je presek konveksnih množic, torej je konveksna množica.
    \end{proof}

    \begin{proof}[Dokaz \emph{(3)}]
        \(C(A)\) je presek vseh konveksnih množic, ki vsebujejo \(A\). Med njimi je tudi \(B\). Torej je \(C(A) \subseteq B\).
    \end{proof}

    \begin{proof}[Dokaz \emph{(4)}]
        \(C(A)\) je presek vseh konveksnih množic, ki vsebujejo \(A\). Ker je \(A\) konveksna, je \(A\) tudi ena izmed teh množic, in ker je vsebovana v vseh ostalih množicah v preseku, je \(C(A)=A\).
    \end{proof}

    \begin{proof}[Dokaz \emph{(5)}]
        \[
            (1-\lambda) \sum_{i=1}^k \lambda_i x_i + \lambda \sum_{i=1}^k \lambda_i^\prime x_i^\prime    
        \]
        je konveksna kombinacija točk iz množice konveksnih kombinacij tok iz \(A\):
        \[
            (1-\lambda) \sum_{i=1}^k \lambda_i + \lambda \sum_{i=1}^k \lambda_i^\prime = (1-\lambda) \cdot 1 + \lambda \cdot 1 = 1    
        \]
        Množica konveksnih kombinacij točk iz \(A\) je torej konveksna, kar pomeni, da vsebuje \(A\). \\
        Naj bodo \(x_1,...,x_k \in A \subseteq C(A)\). Ker je \(C(A)\) konveksna, je tudi vsaka konveksna kombinacija \(\lambda_1 x_1 + ... + \lambda_k x_k \in C(A)\), torej \(C(A)\) vsebuje vse konveksne kombinacije točk iz \(A\). Ker pa množica konveksnih kombinacij vsebuje \(A\), sledi, da sta \(C(A)\) in množica konveksnih kombinacij točk iz \(A\) enaki.
    \end{proof}

    \begin{definition}
        \(x\) je \textbf{ekstremna točka} v konveksni množici \(K\), če ne obstajajo \(x_1,x_2 \in K\), \(\lambda \in (0,1)\):
        \[
            x = (1-\lambda) x_1 + \lambda x_2    
        \]
        Torej, ekstremna točka ne leži v notranosti nobene zveznice med dvema točkama v \(K\).
    \end{definition}

    \begin{definition}
        \textbf{Afina kombinacija} dveh točk \(x,y\) je \((1-\lambda) x + \lambda y\) za vse \(\lambda \in \mathbb{R}\). \\
        Ekvivalentno, \(\lambda x + \mu y\) za \(\lambda, \mu \in \mathbb{R}\), za katera je \(\lambda + \mu = 1\).
        Afina kombinacija točk \(x_1,...,x_k\) je \(\lambda_1 x_1 + ... + \lambda_k x_k\), kjer je \(\lambda_1 + ... + \lambda_k = 1\).
    \end{definition}

    \begin{definition}
        Množica \(A\) je \textbf{afina}, če je za vse \(x,y \in A\) in \(\lambda \in \mathbb{R}\): \((1-\lambda) x + \lambda y \in A\).
    \end{definition}

    \begin{proposition}
        Naslednje trditve so ekvivalentne:
        \begin{enumerate}[label=(\arabic*)]
            \item \(A\) je afina.
            \item Vsaka afina kombinacija točk iz \(A\) je v \(A\).
            \item \(A = V + a\), \(V \le \mathbb{R}^n\) vektorski podprostor
        \end{enumerate}
    \end{proposition}

    \begin{proof}[Dokaz \emph{(1)} \(\implies\) \emph{(2)}]
        Dokažimo z indukcijo na številu členov v kombinaciji \(k\): za \(k=1\) in \(k=2\) že vemo. Predpostavimo, da je afina kombinacija s \(k-1\) členi iz \(A\) v \(A\). \\
        Poglejmo afino kombinacijo \(\lambda_1 x_1 + ... + \lambda_k x_k\). Brez škode za splošnost predpostavimo, da \(\lambda_k \neq 1\). 
        \[
            (1-\lambda_k) (\frac{\lambda_1}{1-\lambda_k} x_1 + ... + \frac{\lambda_{k-1}}{1-\lambda_k} x_{k-1}) + \lambda_k x_k \in A    
        \]

    \end{proof}

    \begin{proof}[Dokaz \emph{(2)} \(\implies\) \emph{(3)}]
        Vzemimo poljuben \(a \in A\). Definiramo \(V := \{\ x-a\ |\ x \in A\ \}\). Dokazati moramo, da je \(V\) vektorski podprostor v \(\mathbb{R}^n\). \\
        \[
            \lambda (x-a) + \mu (y-a) = \lambda x + \mu y + (\lambda + \mu) a = \lambda x + \mu y + (1 - \lambda - \mu) a - a \in A - a = V     
        \]
    \end{proof}

    \begin{proof}[Dokaz \emph{(3)} \(\implies\) \emph{(1)}]
        \(A = V  + a\) \\
        \[
            (1-\lambda) (x+a) + \lambda (y+a) = (1-\lambda) x + \lambda y + (1-\lambda) a + \lambda a = (1-\lambda) x + \lambda y + a \in A
        \]
    \end{proof}



    \subsection{Konveksni stožci in Farkaseva lema}

    \begin{definition}
        Množica \(S \subseteq \mathbb{R}^n\) je \textbf{konveksni stožec}, če za vse \(x,y \in S\), \(\lambda, \mu \ge 0\) velja \(\lambda x + \mu y \in S\).
    \end{definition}

    \begin{remark}
        Konveksni stožev je konveksna množica.
    \end{remark}

    \begin{definition}
        \(S(a_1,...a_k) = \{\ \lambda_1 a_1 + ... + \lambda_k a_k\ |\ \lambda_1,...,\lambda_k \ge 0\ \}\)
    \end{definition}

    \begin{proposition}
        \(S(a_1,...,a_k)\) je konveksni stožec. Imenujemo ga konveksni stožec, ki ga napenjajo \(a_1,...a_k\).
    \end{proposition}

    \begin{proof}
        \[
            \lambda (\lambda_1 a_1 + ... + \lambda_k a_k) + \mu (\mu_1 a_1 + ... + \mu_k a_k)
        \]
        Za vse \(i\) je \(\lambda \lambda_i + \mu \mu_i \ge 0\).
        \[
            (\lambda \lambda_1 + \mu \mu_1) a_1 + ... + (\lambda \lambda_k + \mu \mu_k) a_k \in S(a_1,...,a_k)
        \]
    \end{proof}

    \begin{definition}
        Naj bo \(A \subseteq \mathbb{R}^n\).
        \[
            A^* = \{\ x \in \mathbb{R}^n\ |\ x^T a \ge 0\ \text{za vse}\ a \in A\ \}
        \]
        \(A^*\) vsebuje vse vektorje, ki tvorijo ostri kot \(\phi \in [-\pi / 2, \pi / 2]\) z vsemi vektorji iz \(A\).
    \end{definition}

    \begin{proposition}
        \(A^*\) je konveksni stožec. Ienujemo ga \textbf{dualni stožec}.
    \end{proposition}

    \begin{proof}
        Vzamemo poljubne \(x,y \in A^*\), \(\lambda, \mu \ge 0\). Potem za vse \(a \in A\) velja:
        \[
            (\lambda x + \mu y)^T a = \lambda x^T a + \mu y^T a \ge 0
        \]
    \end{proof}

    \begin{proposition}
        \(S^*(a_1,...,a_k) = \{\ x \in \mathbb{R}^n\ |\ x^T a_i \ge 0\ \text{za}\ i=1,..,k\ \}\)
    \end{proposition}

    \begin{proof}
        Za vse \(x\) in \(\lambda_1,...,\lambda_k \ge 0\) velja
        \[
            x^T (\lambda_1 a_1 + ... + \lambda_k a_k) = \lambda_1 x^T a_1 + ... + \lambda_k x^T a_k \ge 0
        \]
        Torej \(S^*(a_1,...,a_k)\) vsebuje desno stran.
        Ker so vse \(\lambda_i \ge 0\), je tudi \(x^T a_i \ge 0\) za \(i=1,...,k\), torej tudi leva stran vsebuje desno. Med levo in desno stranjo velja enakost.
    \end{proof}

    \begin{remark}
        V splošnem \(A^{**} \neq A\), velja pa \(A \subseteq A^{**}\).
    \end{remark}

    \begin{theorem}[Farkaseva lema, geometrijska oblika]
        \(S^{**}(a_1,...,a_k)=S(a_1,...,a_k)\)
    \end{theorem}

    \begin{proof}
        Da \(S(a_1,...,a_k) \subseteq S^{**}(a_1,...,a_k)\), že vemo. Pokazati moramo še obratno vsebovanost. \\

    \end{proof}
\end{document}